{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/custom-env/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import wave\n",
    "import pyroomacoustics as pa\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 乱数を初期化\n",
    "seed = 1\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声データを指定したサンプリングレートで保存\n",
    "def save_audio_file(file_path, data, sample_rate):\n",
    "    sf.write(file_path, data, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声に室内インパルス応答（Room Impulse Response）を畳み込む\n",
    "def rir_convolve(wave_files, sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, gain_decay, max_order=0, absorption=None, SNR=None, noise_supression=False):\n",
    "    \"\"\"\n",
    "    wave_files: シングルチャンネルの音声のパスを格納したリスト\n",
    "    sample_rate: サンプリング周波数 [Hz]\n",
    "    audio_length: 音声の長さ [sec]\n",
    "    doas: 音源の到来方向\n",
    "    distance_mic_to_source: 音源とマイクロホンの距離 [m]\n",
    "    mic_array_loc: マイクロホンアレイの位置座標\n",
    "    R: 各マイクロホンの空間的な座標\n",
    "    room_dim: 部屋の３次元形状を表す（単位はm）\n",
    "    gain_decay: 雑音の減衰率（雑音が大きすぎるため）\n",
    "    max_order: 部屋の壁で何回音が反射するか（反射しない場合0）\n",
    "    absorption: 部屋の壁でどの程度音が吸収されるか （吸収されない場合None）\n",
    "    SNR: 音声と雑音の比率 [dB]\n",
    "    noise_supression: 雑音の音量が下げる場合True\n",
    "    \"\"\"\n",
    "    n_sources = len(wave_files)\n",
    "#     print(\"音源数:\", n_sources)\n",
    "    source_locations = np.zeros((3, doas.shape[0]), dtype=doas.dtype)\n",
    "    \"\"\"source_locations: (xyz, num_sources)\"\"\"\n",
    "    \n",
    "    source_locations[0,  :] = np.cos(doas[:, 1]) * np.cos(doas[:, 0]) # x = rcosφcosθ\n",
    "    source_locations[1,  :] = np.sin(doas[:, 1]) * np.cos(doas[:, 0]) # y = rsinφcosθ\n",
    "    source_locations[2,  :] = np.sin(doas[:, 0]) # z = rsinθ\n",
    "    source_locations *= distance_mic_to_source\n",
    "    source_locations += mic_array_loc[:, None] # マイクロホンアレイからの相対位置→絶対位置\n",
    "    for i in range(n_sources):\n",
    "        x = source_locations[0, i]\n",
    "        y = source_locations[1, i]\n",
    "        z = source_locations[2, i]\n",
    "#         print(\"{}個目の音源の位置： (x, y, z) = ({}, {}, {})\".format(i+1, x, y, z))\n",
    "\n",
    "    # 音源数分の音声ファイルを読み込む\n",
    "    for s, wave_file in enumerate(wave_files):\n",
    "        audio_data, _ = sf.read(wave_file)\n",
    "        \"\"\"audio_data: (num_samples, )\"\"\"\n",
    "        # 雑音の音量を小さくする場合\n",
    "        if noise_supression:\n",
    "            audio_data *= gain_decay\n",
    "        if s == 0:\n",
    "            clean_data = audio_data[np.newaxis, :]\n",
    "        else:\n",
    "            # 混合音声中の背景雑音の音量を若干小さくする（臨時処理） TODO\n",
    "            if s == 2:\n",
    "                audio_data *= gain_decay\n",
    "            # 目的話者の発話と干渉話者の発話の長さが違う場合は目的話者の発話の長さに合わせる\n",
    "            if clean_data.shape[1] < len(audio_data):\n",
    "                audio_data = audio_data[:clean_data.shape[1]]\n",
    "            elif clean_data.shape[1] > len(audio_data):\n",
    "                audio_data = np.pad(audio_data, (0, max(0, clean_data.shape[1] - len(audio_data))), \"constant\")\n",
    "            clean_data = np.append(clean_data, audio_data[np.newaxis, :], axis=0)\n",
    "    \"\"\"clean_data: (num_sources, num_samples)\"\"\"\n",
    "        \n",
    "    # 部屋を生成する\n",
    "    room = pa.ShoeBox(room_dim, fs=sample_rate, max_order=max_order, absorption=absorption)\n",
    "    # 用いるマイクロホンアレイの情報を設定する\n",
    "    room.add_microphone_array(pa.MicrophoneArray(R, fs=room.fs))\n",
    "    # 各音源をシミュレーションに追加する\n",
    "    for s in range(n_sources):\n",
    "        clean_data[s] /= np.std(clean_data[s])\n",
    "        # たまに「ValueError: The source must be added inside the room.」が出る\n",
    "        room.add_source(source_locations[:, s], signal=clean_data[s])\n",
    "    # RIRのシミュレーション生成と音源信号への畳み込みを実行\n",
    "    room.simulate(snr=SNR)\n",
    "    \n",
    "#     # インパルス応答の取得と残響時間（RT60）の取得\n",
    "#     impulse_responses = room.rir\n",
    "#     rt60 = pa.experimental.measure_rt60(impulse_responses[0][0], fs=sample_rate)\n",
    "#     print(\"残響時間:{} [sec]\".format(rt60))\n",
    "\n",
    "    # 室内インパルス応答を畳み込んだ波形データを取得\n",
    "    convolved_wave = room.mic_array.signals.T\n",
    "    \"\"\"convolved_wave: (num_samples, num_channels)\"\"\"\n",
    "    \n",
    "    return convolved_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/3500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "部屋の3次元形状： [5. 5. 5.]\n",
      "マイクロホンアレイ中心座標： [2.5  2.5  0.53]\n",
      "マイクロホン数： 8\n",
      "オリジナルデータの数: 5689\n",
      "trainデータの数: 3500\n",
      "validationデータの数: 350\n",
      "testデータの数: 30\n",
      "trainデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [3:30:26<00:00,  3.61s/it]  \n",
      "  0%|          | 0/350 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validationデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [21:09<00:00,  3.63s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:48<00:00,  3.60s/it]\n",
      "100%|██████████| 30/30 [01:48<00:00,  3.61s/it]\n",
      "100%|██████████| 30/30 [01:47<00:00,  3.59s/it]\n",
      "100%|██████████| 30/30 [01:48<00:00,  3.60s/it]\n",
      "100%|██████████| 30/30 [01:48<00:00,  3.62s/it]\n",
      "100%|██████████| 30/30 [01:48<00:00,  3.61s/it]\n",
      "100%|██████████| 30/30 [01:48<00:00,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ作成完了　保存先：../data/NoisySpeechDataset_multi_wav_test_original_length_two_speakers_rt0161_20210627/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 各パラメータを設定\n",
    "    sample_rate = 16000 # 作成するオーディオファイルのサンプリング周波数を指定\n",
    "    audio_length = 3 # 単位は秒(second) → fft_size=1024,hop_length=768のとき、audio_length=6が最適化かも？\n",
    "    train_val_ratio = 0.9 # trainデータとvalidationデータの割合\n",
    "    fft_size = 512 # 短時間フーリエ変換のフレーム長\n",
    "    hop_length = 160 # 短時間フーリエ変換においてフレームをスライドさせる幅\n",
    "    gain_decay = 0.7 # 音量調整のためのパラメータ（雑音が大きすぎるため）\n",
    "    \n",
    "    # RIR生成用のパラメータ\n",
    "    # 畳み込みに用いる波形\n",
    "    # clean_wave_files = [\"../data/NoisySpeechDatabase/noisy_trainset_28spk_wav_16kHz/p230_013.wav\"]\n",
    "    # 音声と雑音の比率 [dB]\n",
    "    SNR = None\n",
    "    # 音源とマイクロホンの距離 [m]\n",
    "    distance_mic_to_source=2\n",
    "#     # 音源方向（音源が複数ある場合はリストに追加）\n",
    "#     azimuth = [0] # 方位角\n",
    "#     elevation = [np.pi/6] # 仰角\n",
    "    noise_azimuth = np.pi # 雑音の方位角\n",
    "    # 部屋（シミュレーション環境）の設定\n",
    "    room_width = 5.0\n",
    "    room_length = 5.0\n",
    "    room_height = 5.0\n",
    "    # 部屋の残響を設定（以下は目安）\n",
    "    # max_order = 8, absorption=0.524で残響時間は0.1003750\n",
    "    # max_order=22, absorption=0.333で残響時間は0.2002499\n",
    "    # max_order=27, absorption=0.206で残響時間は0.3003125\n",
    "    # max_order=36, absorption=0.1565で残響時間は0.4008125\n",
    "    # max_order=48, absorption=0.1386で残響時間は0.5000625\n",
    "    max_order = 27 # 部屋の壁で何回音が反射するか（反射しない場合0）\n",
    "    absorption = 0.206 # 部屋の壁でどの程度音が吸収されるか （吸収されない場合None）\n",
    "    # 以下は固定\n",
    "    # 部屋の３次元形状を表す（単位はm）\n",
    "    room_dim = np.r_[room_width, room_length, room_height]\n",
    "    print(\"部屋の3次元形状：\", room_dim)\n",
    "    # マイクロホンアレイの中心位置\n",
    "    nakbot_height = 0.57 # Nakbotの全長\n",
    "    mic_array_height = nakbot_height - 0.04 # 0.04はTAMAGO-03マイクロホンアレイの頂上部からマイクロホンアレイ中心までの距離\n",
    "    mic_array_loc = np.r_[room_width/2, room_length/2, 0] + [0, 0, mic_array_height] # 部屋の中央に配置されたNakbot上のマイクロホンアレイ\n",
    "    print(\"マイクロホンアレイ中心座標：\", mic_array_loc)\n",
    "    # TAMAGO-03のマイクロホンアレイのマイクロホン配置（単位はm）\n",
    "    mic_alignments = np.array(\n",
    "    [\n",
    "        [0.035, 0.0, 0.0],\n",
    "        [0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, 0.035, 0.0],\n",
    "        [-0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [-0.035, 0.0, 0.0],\n",
    "        [-0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, -0.035, 0.0],\n",
    "        [0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0]\n",
    "    ])\n",
    "    n_channels = np.shape(mic_alignments)[0]\n",
    "    print(\"マイクロホン数：\", n_channels)\n",
    "    # get the microphone array （各マイクロホンの空間的な座標）\n",
    "    R = mic_alignments.T + mic_array_loc[:, None]\n",
    "    \"\"\"R: (3D coordinates [m], num_microphones)\"\"\"\n",
    "#     # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "#     doas = np.array(\n",
    "#     [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#     # [elevation[1], azimuth[1]] # ２個目の音源\n",
    "#     ])\n",
    "    \n",
    "    # データセットを格納するディレクトリを作成\n",
    "    save_dataset_dir = \"../data/NoisySpeechDataset_multi_wav_test_two_speakers_rt0300/\"\n",
    "    os.makedirs(save_dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # 元の長さ版\n",
    "    # 目的話者の発話のディレクトリを指定\n",
    "    target_data_dir_for_test= \"../data/NoisySpeechDatabase/clean_testset_wav_16kHz_original_length_half1/\"\n",
    "    # 干渉話者の発話のディレクトリを指定\n",
    "    interference_data_dir_for_test = \"../data/NoisySpeechDatabase/clean_testset_wav_16kHz_original_length_half2/\"\n",
    "    # 外部雑音のディレクトリを指定\n",
    "    noise_data_dir_for_test = \"../data/NoisySpeechDatabase/interference_testset_wav_16kHz_original_length/\"\n",
    "    # 混合音声のディレクトリを指定\n",
    "    mixed_data_dir_for_test = \"../data/NoisySpeechDatabase/noisy_testset_wav_16kHz_original_length/\"\n",
    "    \n",
    "    # テストデータのリストを作成\n",
    "    # 目的話者の発話データ\n",
    "    target_data_path_template_for_test = os.path.join(target_data_dir_for_test, \"*.wav\")\n",
    "    target_list_for_test = glob.glob(target_data_path_template_for_test)\n",
    "    random.seed(seed)\n",
    "    target_list_for_test = random.sample(target_list_for_test, 30) # データ量削減\n",
    "    # 干渉話者の発話データ\n",
    "    interference_data_path_template_for_test = os.path.join(interference_data_dir_for_test, \"*.wav\")\n",
    "    interference_list_for_test = glob.glob(interference_data_path_template_for_test)\n",
    "#     random.seed(seed)\n",
    "#     target_list_for_test = random.sample(target_list_for_test, 30) # データ量削減\n",
    "    print(\"testデータの数:\", len(target_list_for_test))\n",
    "        \n",
    "    # testデータを作成\n",
    "    print(\"testデータ作成中\")\n",
    "    test_data_path = os.path.join(save_dataset_dir, \"test\")\n",
    "    os.makedirs(test_data_path, exist_ok=True)\n",
    "    \n",
    "    # 干渉音の到来方向を指定（0°, 15°, 30°, 45°, 60°, 75°, 90°の7分割）\n",
    "    interference_azimuth_list = [i * (np.pi / 12) for i in range(7)]\n",
    "    \n",
    "    for interference_azimuth_idx, interference_azimuth in enumerate(interference_azimuth_list): \n",
    "        for idx, target_path in enumerate(tqdm(target_list_for_test)):\n",
    "            file_num = os.path.basename(target_path).split('.')[0] # (例)p226_001\n",
    "            target_file_name = file_num + \"_target.wav\" # (例)p226_001_target.wav\n",
    "\n",
    "            # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "            # 目的音の畳み込み\n",
    "            # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "            azimuth = [0] # 方位角（1個目の音源, 2個目の音源）\n",
    "#             azimuth = [np.pi]\n",
    "            elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "            # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "            doas = np.array(\n",
    "            [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "    #         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "            ])\n",
    "            convolved_target_data = rir_convolve([target_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                     mic_array_loc, R, room_dim, gain_decay, max_order=0, absorption=None, SNR=None, noise_supression=False)\n",
    "\n",
    "            # 干渉音の畳み込み\n",
    "#             interference_path = os.path.join(interference_data_dir_for_test, file_num + \".wav\")    \n",
    "            interference_path = random.sample(interference_list_for_test, 1)[0]\n",
    "            interference_file_num = os.path.basename(interference_path).split('.')[0] # (例)p226_001\n",
    "    #         # 干渉音の到来方向を指定（0°, 15°, 30°, 45°, 60°, 75°, 90°の7分割）\n",
    "    #         interference_azimuth = int(idx / (len(target_list_for_test) / 7)) * (np.pi / 12)\n",
    "            # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "            azimuth = [interference_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "            elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "            # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "            doas = np.array(\n",
    "            [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "    #         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "            ])\n",
    "            # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "            convolved_interference_data = rir_convolve([interference_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                     mic_array_loc, R, room_dim, gain_decay, max_order=0, absorption=None, SNR=None, noise_supression=False)\n",
    "\n",
    "            # 雑音の畳み込み\n",
    "            noise_path = os.path.join(noise_data_dir_for_test, file_num + \".wav\")\n",
    "            # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "            elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "            # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "            doas = np.array(\n",
    "            [[elevation[0], noise_azimuth], # １個目の音源 \n",
    "    #         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "            ])\n",
    "            # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "            convolved_noise_data = rir_convolve([noise_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                     mic_array_loc, R, room_dim, gain_decay, max_order=0, absorption=None, SNR=None, noise_supression=True)\n",
    "\n",
    "            # 畳み込む音声をリストに格納\n",
    "            wave_files = [target_path, interference_path, noise_path]\n",
    "            # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "            azimuth = [0, interference_azimuth, noise_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "#             azimuth = [np.pi, interference_azimuth] # 20210226一時的に変更\n",
    "            elevation = [np.pi/6, np.pi/6, np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "            # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "            doas = np.array(\n",
    "            [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "            [elevation[1], azimuth[1]], # ２個目の音源\n",
    "            [elevation[2], azimuth[2]] # 3個目の音源\n",
    "            ])\n",
    "            # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "            convolved_mixed_data = rir_convolve(wave_files, sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                     mic_array_loc, R, room_dim, gain_decay, max_order=max_order, absorption=absorption, SNR=SNR, noise_supression=False)\n",
    "            # RIRの長さ-1サンプル分音声データが長くなるので、convolved_target_dataの長さに揃える\n",
    "            convolved_mixed_data = convolved_mixed_data[:convolved_target_data.shape[0], :]\n",
    "            \"\"\"convolved_mixed_data: (num_samples, num_channels=8)\"\"\"\n",
    "\n",
    "            # 混合音声の最大振幅で正規化\n",
    "            normalized_convolved_target_data = convolved_target_data / convolved_mixed_data.max()\n",
    "            normalized_convolved_interference_data = convolved_interference_data / convolved_mixed_data.max()\n",
    "            normalized_convolved_noise_data = convolved_noise_data / convolved_mixed_data.max()\n",
    "            normalized_convolved_mixed_data = convolved_mixed_data / convolved_mixed_data.max()\n",
    "            \n",
    "            # 音声データのサンプリング周波数を指定して保存\n",
    "            # 干渉音の到来方向ごとに保存\n",
    "#             interference_azimuth = str(int(idx / (len(target_list_for_test) / 7)) * 15) # 0, 15, 30・・・,90\n",
    "            dir_name_interference_azimuth_wise = str(interference_azimuth_idx * 15)\n",
    "            save_dir_azimuth_wise = os.path.join(test_data_path, dir_name_interference_azimuth_wise)\n",
    "            os.makedirs(save_dir_azimuth_wise, exist_ok=True)\n",
    "            # 目的音\n",
    "            target_file_path = os.path.join(save_dir_azimuth_wise, target_file_name)\n",
    "            save_audio_file(target_file_path, normalized_convolved_target_data, sample_rate)\n",
    "            # 干渉音\n",
    "            interference_file_name = file_num +\"_\" + interference_file_num + \"_interference.wav\" # (例)p226_001_p258_041_interference.wav\n",
    "            interference_file_path = os.path.join(save_dir_azimuth_wise, interference_file_name)\n",
    "            save_audio_file(interference_file_path, normalized_convolved_interference_data, sample_rate)\n",
    "            # 雑音\n",
    "            noise_file_name = file_num +\"_\" + interference_file_num + \"_noise.wav\" # (例)p226_001_p258_041_noise.wav\n",
    "            noise_file_path = os.path.join(save_dir_azimuth_wise, noise_file_name)\n",
    "            save_audio_file(noise_file_path, normalized_convolved_noise_data, sample_rate)\n",
    "            # 混合音声\n",
    "            mixed_file_name = file_num +\"_\" + interference_file_num + \"_mixed.wav\" # (例)p226_001_p258_041_mixed.wav\n",
    "            mixed_file_path = os.path.join(save_dir_azimuth_wise, mixed_file_name)\n",
    "            save_audio_file(mixed_file_path, normalized_convolved_mixed_data, sample_rate)\n",
    "\n",
    "    print(\"データ作成完了　保存先：{}\".format(save_dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "custom-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
