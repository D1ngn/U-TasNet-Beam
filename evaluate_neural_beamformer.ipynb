{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要モジュールのimport\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.signal as signal\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "from models import FCMaskEstimator, BLSTMMaskEstimator, BLSTMMaskEstimator2, UnetMaskEstimator_kernel3, \\\n",
    "    UnetMaskEstimator_kernel3_single_mask, CNNMaskEstimator_kernel3, UnetMaskEstimator_kernel3_single_mask_two_speakers, \\\n",
    "    UnetMaskEstimator_kernel3_single_mask_dereverb, MCComplexUnet, MCConvTasNet\n",
    "from beamformer import estimate_covariance_matrix, condition_covariance, estimate_steering_vector, sparse, \\\n",
    "    ds_beamformer, mvdr_beamformer, mvdr_beamformer_two_speakers, gev_beamformer, mwf, localize_music, estimate_covariance_matrix_sig\n",
    "from utils.utilities import AudioProcessForComplex, wave_plot\n",
    "from utils.loss_func import solve_inter_channel_permutation_problem\n",
    "# 話者識別用モデル\n",
    "from utils.embedder import SpeechEmbedder\n",
    "# 音源分離用モジュール \n",
    "from asteroid.models import BaseModel\n",
    "\n",
    "sys.path.append('..')\n",
    "from MyLibrary.MyFunc import audio_eval, ASR, asr_eval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 各パラメータを設定\n",
    "    sample_rate = 16000 # 作成するオーディオファイルのサンプリング周波数を指定\n",
    "    fft_size = 512 # 高速フーリエ変換のフレームサイズ\n",
    "    hop_length = 160 # 高速フーリエ変換におけるフレームのスライド幅\n",
    "    # マスクのチャンネルを指定（いずれはconfigまたはargsで指定）TODO\n",
    "    target_aware_channel = 0\n",
    "    noise_aware_channel = 4\n",
    "    # 音声をバッチ処理する際の1バッチ当たりのサンプル数\n",
    "    batch_length = 48000\n",
    "    # 目的話者の発話位置（マイク正面を0°としたときの水平角）\n",
    "    true_target_azimuth = 0\n",
    "    # 処理後の音声の振幅の最大値を処理前の混合音声の振幅の最大値に合わせる（True）か否か（False）\n",
    "    fit_max_value = False\n",
    "    \n",
    "    #########################音源定位用設定########################\n",
    "    freq_range = [200, 3000] # 空間スペクトルの算出に用いる周波数帯[Hz]\n",
    "    # TAMAGO-03マイクロホンアレイにおける各マイクロホンの空間的な位置関係\n",
    "    mic_alignments = np.array(\n",
    "    [\n",
    "        [0.035, 0.0, 0.0],\n",
    "        [0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, 0.035, 0.0],\n",
    "        [-0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [-0.035, 0.0, 0.0],\n",
    "        [-0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, -0.035, 0.0],\n",
    "        [0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0]\n",
    "    ])\n",
    "    \"\"\"mic_alignments: (num_microphones, 3D coordinates [m])\"\"\"\n",
    "    # 各マイクロホンの空間的な位置関係を表す配列\n",
    "    mic_alignments = mic_alignments.T # get the microphone arra\n",
    "    \"\"\"mic_alignments: (3D coordinates [m], num_microphones)\"\"\"\n",
    "    #############################################################\n",
    "    \n",
    "    # 評価する音声ファイルを格納したディレクトリを指定\n",
    "    test_data_dir = \"./audio_data/NoisySpeechDataset_multi_wav_test_original_length_two_speakers_20210714/test\" # 残響なし、複数話者（最新版）\n",
    "    # test_data_dir = \"./audio_data/NoisySpeechDataset_multi_wav_test_original_length_two_speakers_rt0300_20210714/test\" # 残響あり、複数話者（最新版）\n",
    "#     test_data_dir = \"./audio_data/NoisySpeechDataset_multi_wav_test_original_length_two_speakers_spatial_resolution_check_20210721/test\" # 残響なし、複数話者（0°〜15°で3°刻み）\n",
    "    azimuth_list = natsorted(os.listdir(test_data_dir)) # 0, 15, 30,・・・,90\n",
    "#     azimuth_list.pop(1)\n",
    "    print(\"azimuth_list:\", azimuth_list)\n",
    "    # 音声認識精度評価用正解ラベルを格納したディレクトリを指定\n",
    "    reference_label_dir = \"../data/NoisySpeechDatabase/testset_txt/\"\n",
    "    \n",
    "    # 「https://huggingface.co/models?filter=asteroid」にある話者分離用の学習済みモデルを指定\n",
    "#     pretrained_param_speaker_separation = \"JorisCos/ConvTasNet_Libri2Mix_sepclean_16k\" # ConvTasNet 16kHz\n",
    "#     pretrained_param_speaker_separation = \"JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k\" # ConvTasNet 16kHz noisy ← こっちの方が精度が高そう\n",
    "    # 話者識別用の学習済みモデルのパス\n",
    "    embedder_path = \"./utils/embedder.pt\"\n",
    "    # 声を抽出したい人の発話サンプルのパス\n",
    "    ref_speech_path = \"./utils/ref_speech/sample.wav\"\n",
    "    \n",
    "    # 雑音（残響）除去モデルの種類を指定\n",
    "    denoising_model_type = 'Complex_Unet' # 'FC' or 'BLSTM' or 'Unet' or 'Unet_single_mask' or 'Unet_single_mask_two_speakers' or 'Unet_single_mask_dereverb' or 'Complex_Unet'\n",
    "    # 話者分離モデルの種類を指定\n",
    "    speaker_separation_model_type = 'MCConvTasNet' # 'MCConvTasNet'\n",
    "    \n",
    "    # ビームフォーマの種類を指定\n",
    "    beamformer_type = 'MVDR' # 'DS' or 'MVDR' or 'GEV', or 'MWF' or 'Sparse'\n",
    "    # 残響除去手法の種類を指定\n",
    "    dereverb_type = None # None or 'WPE' or 'WPD'\n",
    "    \n",
    "    # 音声認識結果を保存するディレクトリを指定\n",
    "    recog_result_dir = \"./recog_result/{}_{}_{}_{}_dereverb_{}/\".format(test_data_dir.split('/')[-2], denoising_model_type, speaker_separation_model_type, beamformer_type, str(dereverb_type))\n",
    "    os.makedirs(recog_result_dir, exist_ok=True)\n",
    "    \n",
    "    # モデルの設定\n",
    "    # 学習済みのパラメータを保存したチェックポイントファイルのパスを指定\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_aware_1208/ckpt_epoch110.pt\" # U-Net small\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_aware_1211/ckpt_epoch160.pt\" # U-Net small2\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_all_Unet_aware_1215/ckpt_epoch150.pt\" # U-Net all\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_FC_1201/ckpt_epoch120.pt\" # FC small\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_BLSTM_1201/ckpt_epoch70.pt\" # BLSTM small data（wrong version）\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_BLSTM2_1231/ckpt_epoch100.pt\" # BLSTM2 small data（wrong version）\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_SCM_aware_20210105/ckpt_epoch90.pt\" # U-Net SCM  \n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_aware_20210111/ckpt_epoch100.pt\" # U-Net small data training 1209 dataset (best model)\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_SCM_conditioned_median_20210120/ckpt_epoch110.pt\" # U-Net-SCM median small data training 1209 dataset\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_SCM_raw_conditioned_aware_20210227/ckpt_epoch20.pt\" # U-Net-SCM-raw small data training 1209 dataset\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_CNN_aware_20210310/ckpt_epoch200.pt\" # CNN small data training 1209 dataset\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_BLSTM_median_20210312/ckpt_epoch120.pt\" # BLSTM small data (correct version)\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_single_mask_median_20210315/ckpt_epoch170.pt\" # U-Net-single-mask small data  (best model new version)\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_single_mask_SCM_conditioned_median_20210318/ckpt_epoch150.pt\" # U-Net-single-mask-SCM small data  \n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_two_speakers_Unet_single_mask_median_lr_000001_20210613/ckpt_epoch500.pt\" # U-Net-single-mask-SCM small data 2speakers\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_Unet_single_mask_median_multisteplr00001start_20210701/ckpt_epoch190.pt\"\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_Unet_single_mask_median_dereverb_multisteplr00001start_rt0300_20210723/ckpt_epoch140.pt\"\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_ComplexUnet_multisteplr00001start_20210914/ckpt_epoch70.pt\" # Complex U-Net speech and noise output\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_ComplexUnet_ch_constant_multisteplr00001start_20210916/ckpt_epoch160.pt\" # Complex U-Net speech and noise output ch constant\n",
    "#     checkpoint_path =\"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_ComplexUnet_ch_constant_si_snr_loss_multisteplr00001start_20210917/ckpt_epoch480.pt\" # Complex U-Net speech and noise output ch constant si-snr loss\n",
    "#     checkpoint_path =\"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_ComplexUnet_si_snr_loss_multisteplr00001start_20210920/ckpt_epoch70.pt\" # Complex U-Net speech and noise output si-snr loss\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_ComplexUnet_ch_constant_snr_loss_multisteplr00001start_20210922/ckpt_epoch490.pt\" # Complex U-Net speech and noise output ch constant snr loss\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_ComplexUnet_ch_constant_snr_loss_multisteplr00001start_20210922/ckpt_epoch50.pt\" # Complex U-Net speech and noise output snr loss\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_rt0300_ComplexUnet_ch_constant_snr_loss_multisteplr00001start_20210925/ckpt_epoch190.pt\" # Complex U-Net speech and noise output ch constant snr loss reverberant\n",
    "    checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_ComplexUnet_ch_constant_snr_loss_multisteplr00001start_20210922/ckpt_epoch490.pt\" # Complex U-Net speech and noise output ch constant snr loss\n",
    "    \n",
    "    # ネットワークモデルの定義、チャンネルの選び方の指定、モデル入力時にパディングを行うか否かを指定\n",
    "    # 雑音（残響）除去モデル\n",
    "    if denoising_model_type == 'BLSTM':\n",
    "#         model = BLSTMMaskEstimator()\n",
    "        denoising_model = BLSTMMaskEstimator2()\n",
    "        channel_select_type = 'median'\n",
    "        padding = False\n",
    "    elif denoising_model_type == 'FC':\n",
    "        denoising_model = FCMaskEstimator()\n",
    "        channel_select_type = 'aware'\n",
    "        padding = False\n",
    "    elif denoising_model_type == 'CNN':\n",
    "        denoising_model = CNNMaskEstimator_kernel3()\n",
    "        channel_select_type = 'aware'\n",
    "        padding = True\n",
    "    elif denoising_model_type == 'Unet':\n",
    "        denoising_model = UnetMaskEstimator_kernel3()\n",
    "        channel_select_type = 'aware'\n",
    "        padding = True\n",
    "    elif denoising_model_type == 'Unet_single_mask':\n",
    "        denoising_model = UnetMaskEstimator_kernel3_single_mask()\n",
    "        channel_select_type = 'single'\n",
    "        padding = True\n",
    "    elif denoising_model_type == 'Unet_single_mask_two_speakers':\n",
    "        denoising_model = UnetMaskEstimator_kernel3_single_mask_two_speakers()\n",
    "        channel_select_type = 'single'\n",
    "        padding = True\n",
    "    elif denoising_model_type == 'Unet_single_mask_dereverb':\n",
    "        denoising_model = UnetMaskEstimator_kernel3_single_mask_dereverb()\n",
    "        channel_select_type = 'single'\n",
    "        padding = True\n",
    "    elif denoising_model_type == 'Complex_Unet':\n",
    "        denoising_model = MCComplexUnet()\n",
    "        channel_select_type = 'single'\n",
    "        padding = True\n",
    "    # 話者分離モデル\n",
    "    if speaker_separation_model_type == 'MCConvTasNet':\n",
    "#         checkpoint_path_for_speaker_separation_model = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_for_ConvTasnet_snr_loss_multisteplr00001start_20210928/ckpt_epoch370.pt\"\n",
    "        checkpoint_path_for_speaker_separation_model = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_for_ConvTasnet_snr_loss_multisteplr00001start_20210928/ckpt_epoch560.pt\" # best model\n",
    "#         checkpoint_path_for_speaker_separation_model = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_for_ConvTasnet_snr_loss_multisteplr00001start_20210928/ckpt_epoch630.pt\"\n",
    "        speaker_separation_model = MCConvTasNet()\n",
    "        \n",
    "    # 音声処理クラスのインスタンスを作成\n",
    "#     audio_processor = AudioProcess(sample_rate, fft_size, hop_length, channel_select_type, padding)\n",
    "    audio_processor = AudioProcessForComplex(sample_rate, fft_size, hop_length, padding)\n",
    "    # GPUが使える場合はGPUを使用、使えない場合はCPUを使用\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\" , device)\n",
    "    # 学習済みのパラメータをロード\n",
    "    denoising_model_params = torch.load(checkpoint_path, map_location=device)\n",
    "    denoising_model.load_state_dict(denoising_model_params['model_state_dict'])\n",
    "    denoising_model.to(device) # モデルをCPUまたはGPUへ\n",
    "    denoising_model.eval() # ネットワークを推論モードへ\n",
    "    # print(\"モデルのパラメータ数：\", count_parameters(model))\n",
    "    # 話者分離モデルの学習済みパラメータをロード\n",
    "    speaker_separation_model_params = torch.load(checkpoint_path_for_speaker_separation_model, map_location=device)\n",
    "    speaker_separation_model.load_state_dict(speaker_separation_model_params['model_state_dict'])\n",
    "    speaker_separation_model.to(device) # モデルをCPUまたはGPUへ\n",
    "    speaker_separation_model.eval() # ネットワークを推論モードへ\n",
    "    # 話者識別モデルの学習済みパタメータをロード（いずれはhparamsでパラメータを指定できる様にする TODO）\n",
    "    embedder = SpeechEmbedder()\n",
    "    embed_params = torch.load(embedder_path, map_location=device)\n",
    "    embedder.load_state_dict(embed_params)\n",
    "    embedder.to(device) # モデルをCPUまたはGPUへ\n",
    "    embedder.eval()\n",
    "    # 声を分離抽出したい人の発話サンプルをロード\n",
    "    ref_speech_data, _ = sf.read(ref_speech_path)\n",
    "    # 発話サンプルの特徴量（ログメルスペクトログラム）をベクトルに変換\n",
    "    ref_complex_spec = audio_processor.calc_complex_spec(ref_speech_data)\n",
    "    ref_log_mel_spec = audio_processor.calc_log_mel_spec(ref_complex_spec)\n",
    "    ref_log_mel_spec = torch.from_numpy(ref_log_mel_spec).float()\n",
    "    ref_dvec = embedder(ref_log_mel_spec[0]) # 入力は1ch分\n",
    "    # PyTorchのテンソルからnumpy配列に変換\n",
    "    ref_dvec = ref_dvec.detach().numpy().copy() # CPU\n",
    "    \"\"\"ref_dvec: (embed_dim=256,)\"\"\"\n",
    "    # 音声認識用のインスタンスを生成\n",
    "    asr_ins = ASR(lang='eng')\n",
    "    \n",
    "    # 評価結果のログを保存するリスト\n",
    "    eval_logs = []\n",
    "    # 干渉音の到来方向ごとに評価\n",
    "    for interference_azimuth in azimuth_list:\n",
    "        ######################雑音除去＋音声評価#########################\n",
    "        # 音声評価結果の合計値を格納するリストを用意\n",
    "        sdr_mix_list = []\n",
    "        sir_mix_list = []\n",
    "        sar_mix_list = []\n",
    "        sdr_est_list = []\n",
    "        sir_est_list = []\n",
    "        sar_est_list = []\n",
    "        # 音声認識用を追加\n",
    "        wer_clean_list = []\n",
    "        wer_mix_list = []\n",
    "        wer_est_list = []\n",
    "        # 音源定位用に追加\n",
    "        localization_error_target_list = []\n",
    "        localization_error_mixed_list = []\n",
    "        localization_error_estimated_list = []\n",
    "        \n",
    "        # 合計処理時間を測るための変数を用意\n",
    "        processing_duration_sum = 0\n",
    "        # 実時間比（Real TIme Factor）を測るための変数を用意\n",
    "        rtf_sum = 0\n",
    "\n",
    "        mixed_audio_path_list = natsorted(glob.glob(os.path.join(test_data_dir, interference_azimuth, \"*_mixed.wav\"))) # （例）p232_016_mixed.wav\n",
    "        for mixed_audio_path in tqdm(mixed_audio_path_list):\n",
    "            # 処理の開始時間\n",
    "            iter_start_time = time.perf_counter()\n",
    "            # 音声データをロード\n",
    "            mixed_audio_data, _ = sf.read(mixed_audio_path)\n",
    "            \"\"\"mixed_audio_data: (num_samples, num_channels)\"\"\"\n",
    "            mixed_complex_spec = audio_processor.calc_complex_spec(mixed_audio_data)\n",
    "            \"\"\"mixed_complex_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "            # 処理前の混合音の音源定位結果を記録（この音源定位処理を処理時間に含めている分、処理後の音源定位処理は処理時間に含めない）\n",
    "            speaker_azimuth_mixed = localize_music(mixed_complex_spec, mic_alignments, sample_rate, fft_size)\n",
    "            localization_error_mixed = np.abs(speaker_azimuth_mixed - true_target_azimuth)\n",
    "            localization_error_mixed_list.append(localization_error_mixed)\n",
    "            \n",
    "            # 残響除去手法を指定している場合は残響除去処理を実行\n",
    "            if dereverb_type == 'WPE':\n",
    "                mixed_complex_spec, _ = audio_processor.dereverberation_wpe_multi(mixed_complex_spec)\n",
    "                \n",
    "            # モデルに入力できるように音声をミニバッチに分けながら振幅＋位相スペクトログラムに変換\n",
    "            # torch.stftを使用する場合\n",
    "            mixed_audio_data_for_model_input = torch.transpose(torch.from_numpy(mixed_audio_data).float(), 0, 1)\n",
    "            \"\"\"mixed_audio_data_for_model_input: (num_channels, num_samples)\"\"\"\n",
    "            mixed_amp_phase_spec_batch = audio_processor.preprocess_mask_estimator(mixed_audio_data_for_model_input, batch_length)\n",
    "            \"\"\"amp_phase_spec_batch: (batch_size, num_channels, freq_bins, time_frames, real_imaginary)\"\"\"\n",
    "#             # librosa.stftを使用する場合\n",
    "#             mixed_amp_phase_spec_batch = audio_processor.preprocess_mask_estimator(mixed_audio_data, batch_length)\n",
    "#             \"\"\"amp_phase_spec_batch: (batch_size, num_channels, freq_bins, time_frames, real_imaginary)\"\"\"\n",
    "            \n",
    "            # 発話とそれ以外の雑音の時間周波数マスクを推定\n",
    "            speech_amp_phase_spec_output, noise_amp_phase_spec_output = denoising_model(mixed_amp_phase_spec_batch)\n",
    "            \"\"\"speech_amp_phase_spec_output: (batch_size, num_channels, freq_bins, time_frames, real_imaginary), \n",
    "            noise_amp_phase_spec_output: (batch_size, num_channels, freq_bins, time_frames, real_imaginary)\"\"\"\n",
    "            # ミニバッチに分けられた振幅＋位相スペクトログラムを時間方向に結合\n",
    "            multichannel_speech_amp_phase_spec= audio_processor.postprocess_mask_estimator(mixed_complex_spec, speech_amp_phase_spec_output, batch_length, target_aware_channel)\n",
    "            \"\"\"multichannel_speech_amp_phase_spec: (num_channels, freq_bins, time_frames, real_imaginary)\"\"\"\n",
    "            multichannel_noise_amp_phase_spec = audio_processor.postprocess_mask_estimator(mixed_complex_spec, noise_amp_phase_spec_output, batch_length, noise_aware_channel)\n",
    "            \"\"\"multichannel_noise_amp_phase_spec: (num_channels, freq_bins, time_frames, real_imaginary)\"\"\"\n",
    "            # torch.stftを使用する場合\n",
    "            # 発話のマルチチャンネルスペクトログラムを音声波形に変換\n",
    "            multichannel_denoised_data = torch.istft(multichannel_speech_amp_phase_spec, n_fft=512, hop_length=160, \\\n",
    "                                                     normalized=True, length=mixed_audio_data.shape[0], return_complex=False)\n",
    "            \"\"\"multichannel_denoised_data: (num_channels, num_samples)\"\"\"\n",
    "            # 雑音のマルチチャンネルスペクトログラムを音声波形に変換\n",
    "            multichannel_noise_data = torch.istft(multichannel_noise_amp_phase_spec, n_fft=512, hop_length=160, \\\n",
    "                                                     normalized=True, length=mixed_audio_data.shape[0], return_complex=False)\n",
    "            \"\"\"multichannel_noise_data: (num_channels, num_samples)\"\"\"\n",
    "#             # librosa.stftを使用する場合\n",
    "#             # 発話のマルチチャンネルスペクトログラムを音声波形に変換\n",
    "#             multichannel_speech_amp_phase_spec = multichannel_speech_amp_phase_spec.detach().numpy().copy() # CPU\n",
    "#             multichannel_speech_complex_spec = multichannel_speech_amp_phase_spec[:, :, :, 0] * np.exp(1j * multichannel_speech_amp_phase_spec[:, :, :, 1])\n",
    "#             multichannel_denoised_data = audio_processor.spec_to_wave(multichannel_speech_complex_spec, mixed_audio_data)\n",
    "#             \"\"\"multichannel_denoised_data: (num_samples, num_channels)\"\"\"\n",
    "\n",
    "            # 話者分離モデルに入力できるようにバッチサイズの次元を追加\n",
    "            multichannel_denoised_data = torch.unsqueeze(multichannel_denoised_data, 0)\n",
    "            \"\"\"multichannel_denoised_data: (batch_size, num_channels, num_samples)\"\"\"\n",
    "            # 話者分離\n",
    "            separated_audio_data = speaker_separation_model(multichannel_denoised_data)\n",
    "            \"\"\"separated_audio_data: (batch_size, num_speakers, num_channels, num_samples)\"\"\"\n",
    "            # チャンネルごとに順序がばらばらな発話の順序を揃える\n",
    "            separated_audio_data = solve_inter_channel_permutation_problem(separated_audio_data)\n",
    "            \"\"\"separated_audio_data: (batch_size, num_speakers, num_channels, num_samples)\"\"\"\n",
    "            \n",
    "            # start_time_speeaker_selector = time.perf_counter()\n",
    "            # PyTorchのテンソルをNumpy配列に変換\n",
    "            separated_audio_data = separated_audio_data.detach().numpy().copy() # CPU\n",
    "            # バッチの次元を消して転置\n",
    "            separated_audio_data = np.transpose(np.squeeze(separated_audio_data, 0), (0, 2, 1))\n",
    "            \"\"\"separated_audio_data: (num_speakers, num_samples, num_channels)\"\"\"\n",
    "            # 分離音から目的話者の発話を選出（何番目の発話が目的話者のものかを判断） →いずれはspeaker_selectorに統一する TODO\n",
    "            target_speaker_id, speech_complex_spec_all = audio_processor.speaker_selector_sig_ver(separated_audio_data, ref_dvec, embedder, device)\n",
    "            \"\"\"speech_complex_spec_all: (num_speakers, num_channels, freq_bins, time_frames)\"\"\"\n",
    "            # print(\"ID of the target speaker:\", target_speaker_id)\n",
    "            # finish_time_speeaker_selector = time.perf_counter()\n",
    "            # duration_speeaker_selector = finish_time_speeaker_selector - start_time_speeaker_selector\n",
    "            # rtf = duration_speeaker_selector / (mixed_audio_data.shape[0] / sample_rate)\n",
    "            # print(\"実時間比（Speaker Selector）：{:.3f}\".format(rtf))\n",
    "\n",
    "            # 目的話者の発話の複素スペクトログラムを取得\n",
    "            multichannel_target_complex_spec = speech_complex_spec_all[target_speaker_id]\n",
    "            \"\"\"multichannel_target_complex_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "            multichannel_interference_complex_spec = np.zeros_like(multichannel_target_complex_spec)\n",
    "            # 干渉話者の発話の複素スペクトログラムを取得\n",
    "            for id in range(speech_complex_spec_all.shape[0]):\n",
    "                # 目的話者以外の話者の複素スペクトログラムを足し合わせる\n",
    "                if id == target_speaker_id:\n",
    "                    pass\n",
    "                else:\n",
    "                    multichannel_interference_complex_spec += speech_complex_spec_all[id]\n",
    "            \"\"\"multichannel_interference_complex_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "            # PyTorchのテンソルをnumpy配列に変換\n",
    "            multichannel_noise_data = multichannel_noise_data.detach().numpy().copy() # CPU\n",
    "            \"\"\"multichannel_noise_data: (num_channels, num_samples)\"\"\"\n",
    "            # 雑音の複素スペクトログラムを算出\n",
    "            multichannel_noise_complex_spec = audio_processor.calc_complex_spec(multichannel_noise_data.T)\n",
    "            \"\"\"multichannel_noise_complex_spec: (num_channels, freq_bins, time_frames)\"\"\" \n",
    "\n",
    "            # 目的音のマスクと雑音のマスクからそれぞれの空間共分散行列を推定\n",
    "            target_covariance_matrix = estimate_covariance_matrix_sig(multichannel_target_complex_spec)\n",
    "            interference_covariance_matrix = estimate_covariance_matrix_sig(multichannel_interference_complex_spec)\n",
    "            noise_covariance_matrix = estimate_covariance_matrix_sig(multichannel_noise_complex_spec)\n",
    "            noise_covariance_matrix = condition_covariance(noise_covariance_matrix, 1e-6) # これがないと性能が大きく落ちる（雑音の共分散行列のみで良い）\n",
    "            # noise_covariance_matrix /= np.trace(noise_covariance_matrix, axis1=-2, axis2=-1)[..., None, None]\n",
    "            # ビームフォーマによる雑音除去を実行\n",
    "            if beamformer_type == 'MVDR':\n",
    "                estimated_target_spec = mvdr_beamformer_two_speakers(mixed_complex_spec, target_covariance_matrix, interference_covariance_matrix, noise_covariance_matrix)\n",
    "                # estimated_interference_spec = mvdr_beamformer_two_speakers(mixed_complex_spec, interference_covariance_matrix, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == 'GEV':\n",
    "                estimated_target_spec = gev_beamformer(mixed_complex_spec, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == \"DS\":\n",
    "                target_steering_vectors = estimate_steering_vector(target_covariance_matrix)\n",
    "                estimated_target_spec = ds_beamformer(mixed_complex_spec, target_steering_vectors)\n",
    "            elif beamformer_type == \"MWF\":\n",
    "                estimated_target_spec = mwf(mixed_complex_spec, target_covariance_matrix, noise_covariance_matrix)\n",
    "            else:\n",
    "                print(\"Please specify the correct beamformer type\")\n",
    "            \"\"\"estimated_target_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "\n",
    "            # マルチチャンネルスペクトログラムを音声波形に変換\n",
    "            multichannel_estimated_target_voice_data = audio_processor.spec_to_wave(estimated_target_spec, mixed_audio_data)\n",
    "            # multichannel_estimated_interference_voice_data = audio_processor.spec_to_wave(estimated_interference_spec, mixed_audio_data)\n",
    "            \"\"\"multichannel_estimated_target_voice_data: (num_samples, num_channels)\"\"\"\n",
    "            \n",
    "#             # 最大値を元の音声に合わせる場合\n",
    "#             if fit_max_value:\n",
    "#                 max_amp_postprocess = multichannel_estimated_voice_data.max()\n",
    "#                 multichannel_estimated_voice_data *= max_amp_preprocess / max_amp_postprocess\n",
    "                \n",
    "            # 処理の終了時間\n",
    "            iter_finish_time = time.perf_counter()\n",
    "            # 1ループ当たりの処理時間（音声波形→STFT→雑音除去→iSTFT→音声波形）\n",
    "            iter_processing_duration = iter_finish_time - iter_start_time\n",
    "            processing_duration_sum += iter_processing_duration\n",
    "            # 実時間比（Real Time Factor）の算出\n",
    "            iter_rtf = iter_processing_duration / (mixed_audio_data.shape[0] / sample_rate)\n",
    "            rtf_sum += iter_rtf\n",
    "            \n",
    "            # 処理後の推定音の定位誤差を記録\n",
    "            speaker_azimuth_estimated = localize_music(estimated_target_spec, mic_alignments, sample_rate, fft_size)\n",
    "            localization_error_estimated = np.abs(speaker_azimuth_estimated - true_target_azimuth)\n",
    "            localization_error_estimated_list.append(localization_error_estimated)\n",
    "            \n",
    "            # ファイル名を取得\n",
    "            file_num = os.path.basename(mixed_audio_path).split('.')[0].rsplit('_', maxsplit=1)[0] # （例） p232_029_p257_236\n",
    "            target_file_num = file_num.rsplit('_', maxsplit=2)[0] # （例） p232_029\n",
    "            # オーディオデータを保存\n",
    "            estimated_target_voice_path = \"./estimated_target_voice_complex_unet_MCConvTasNet.wav\"\n",
    "            # estimated_target_voice_dir = \"./estimated_target_voice_reverberant/{}/\".format(interference_azimuth)\n",
    "            # os.makedirs(estimated_target_voice_dir, exist_ok=True)\n",
    "            # estimated_target_voice_path = os.path.join(estimated_target_voice_dir, \"{}_estimated_target_voice.wav\".format(file_num))\n",
    "            sf.write(estimated_target_voice_path, multichannel_estimated_target_voice_data, sample_rate)\n",
    "            # estimated_interference_voice_path = os.path.join(estimated_target_voice_dir, \"{}_estimated_interference_voice.wav\".format(file_num))            \n",
    "            # sf.write(estimated_interference_voice_path, multichannel_estimated_interference_voice_data, sample_rate)\n",
    "            # 干渉雑音の方位角を取得\n",
    "            target_voice_path = os.path.join(test_data_dir, interference_azimuth, target_file_num + \"_target.wav\") # （例）p232_029_target.wav\n",
    "            interference_audio_path = os.path.join(test_data_dir, interference_azimuth, file_num + \"_interference.wav\") # （例）p232_029_p257_236_interference.wav\n",
    "            \n",
    "            # 処理前の目的話者の定位誤差を記録\n",
    "            target_voice_data, _ = sf.read(target_voice_path)\n",
    "            target_complex_spec = audio_processor.calc_complex_spec(target_voice_data)\n",
    "            speaker_azimuth_target = localize_music(target_complex_spec, mic_alignments, sample_rate, fft_size)\n",
    "            localization_error_target = np.abs(speaker_azimuth_target - true_target_azimuth)\n",
    "            localization_error_target_list.append(localization_error_target)\n",
    "            \n",
    "            # 音声評価\n",
    "            # 音源分離性能の評価        \n",
    "            sdr_mix, sir_mix, sar_mix, sdr_est, sir_est, sar_est = audio_eval(sample_rate, \\\n",
    "            target_voice_path, interference_audio_path, mixed_audio_path, estimated_target_voice_path)\n",
    "            # 音声評価結果を記録\n",
    "            sdr_mix_list.append(sdr_mix)\n",
    "            sir_mix_list.append(sir_mix)\n",
    "            sar_mix_list.append(sar_mix)\n",
    "            sdr_est_list.append(sdr_est)\n",
    "            sir_est_list.append(sir_est)\n",
    "            sar_est_list.append(sar_est)\n",
    "            # 音声認識性能の評価\n",
    "            # 音声認識を実行\n",
    "            target_voice_recog_text = asr_ins.speech_recognition(target_voice_path) # （例） IT IS MARVELLOUS\n",
    "            target_voice_recog_text = target_voice_recog_text.replace('.', '').replace(',', '').upper().split() # （例） ['IT', 'IS', 'MARVELLOUS']\n",
    "            mixed_audio_recog_text = asr_ins.speech_recognition(mixed_audio_path)\n",
    "            mixed_audio_recog_text = mixed_audio_recog_text.replace('.', '').replace(',', '').upper().split()\n",
    "            estimated_voice_recog_text = asr_ins.speech_recognition(estimated_target_voice_path)\n",
    "            estimated_voice_recog_text = estimated_voice_recog_text.replace('.', '').replace(',', '').upper().split()\n",
    "            # 正解ラベルを読み込む\n",
    "            reference_label_path = os.path.join(reference_label_dir, target_file_num + '.txt')\n",
    "            with open(reference_label_path, 'r', encoding=\"utf8\") as ref:\n",
    "                # ピリオドとコンマを消して大文字に変換した後、スペースで分割\n",
    "                reference_label_text = ref.read().replace('.', '').replace(',', '').upper().split()  \n",
    "            # WERを計算\n",
    "            clean_recog_result_save_path = os.path.join(recog_result_dir, file_num + '_clean.txt')\n",
    "            mix_recog_result_save_path = os.path.join(recog_result_dir, file_num + '_mix.txt')\n",
    "            est_recog_result_save_path = os.path.join(recog_result_dir, file_num + '_est.txt')\n",
    "            wer_clean = asr_eval(reference_label_text, target_voice_recog_text, clean_recog_result_save_path)\n",
    "            wer_mix = asr_eval(reference_label_text, mixed_audio_recog_text, mix_recog_result_save_path)\n",
    "            wer_est = asr_eval(reference_label_text, estimated_voice_recog_text, est_recog_result_save_path)\n",
    "            wer_clean_list.append(wer_clean)\n",
    "            wer_mix_list.append(wer_mix)\n",
    "            wer_est_list.append(wer_est)\n",
    "#             # 推定音声が蓄積されないように削除\n",
    "#             os.remove(estimated_target_voice_path)\n",
    "\n",
    "        # データの数を取得\n",
    "        num_file = len(mixed_audio_path_list)\n",
    "        print(\"#\" * 50)\n",
    "        print(\"使用デバイス：\" , device)\n",
    "        print(\"干渉音の方向：{}deg\".format(interference_azimuth))\n",
    "        print(\"合計処理時間：{:.3f}sec\".format(processing_duration_sum))\n",
    "        print(\"平均処理時間：{:.3f}sec\".format(processing_duration_sum/num_file))\n",
    "        print(\"合計実時間比：{:.3f}\".format(rtf_sum))\n",
    "        print(\"平均実時間比：{:.3f}\".format(rtf_sum/num_file))\n",
    "        print(\"============================音源分離性能===============================\")\n",
    "        print(\"平均 | SDR_mix: {:.3f}, SIR_mix: {:.3f}, SAR_mix: {:.3f}\".format(np.mean(sdr_mix_list), np.mean(sir_mix_list), np.mean(sar_mix_list)))\n",
    "        print(\"平均 | SDR_est: {:.3f}, SIR_est: {:.3f}, SAR_est: {:.3f}\".format(np.mean(sdr_est_list), np.mean(sir_est_list), np.mean(sar_est_list)))\n",
    "        print(\"標準偏差 | SDR_mix: {:.3f}, SIR_mix: {:.3f}, SAR_mix: {:.3f}\".format(np.std(sdr_mix_list), np.std(sir_mix_list), np.std(sar_mix_list)))\n",
    "        print(\"標準偏差 | SDR_est: {:.3f}, SIR_est: {:.3f}, SAR_est: {:.3f}\".format(np.std(sdr_est_list), np.std(sir_est_list), np.std(sar_est_list)))\n",
    "        print(\"============================音声認識性能===============================\")\n",
    "        print(\"平均 | WER_clean: {:.3f}\".format(np.mean(wer_clean_list)))\n",
    "        print(\"平均 | WER_mix: {:.3f}\".format(np.mean(wer_mix_list)))\n",
    "        print(\"平均 | WER_est: {:.3f}\".format(np.mean(wer_est_list)))\n",
    "        print(\"標準偏差 | WER_clean: {:.3f}\".format(np.std(wer_clean_list)))\n",
    "        print(\"標準偏差 | WER_mix: {:.3f}\".format(np.std(wer_mix_list)))\n",
    "        print(\"標準偏差 | WER_est: {:.3f}\".format(np.std(wer_est_list)))\n",
    "        print(\"============================音源定位性能===============================\")\n",
    "        print(\"平均 | LE_clean：{:.3f}deg\".format(np.mean(localization_error_target_list)))\n",
    "        print(\"平均 | LE_mix：{:.3f}deg\".format(np.mean(localization_error_mixed_list)))\n",
    "        print(\"平均 | LE_est：{:.3f}deg\".format(np.mean(localization_error_estimated_list)))\n",
    "        print(\"標準偏差 | LE_clean：{:.3f}deg\".format(np.std(localization_error_target_list)))\n",
    "        print(\"標準偏差 | LE_mix：{:.3f}deg\".format(np.std(localization_error_mixed_list)))\n",
    "        print(\"標準偏差 | LE_est：{:.3f}deg\".format(np.std(localization_error_estimated_list)))\n",
    "              \n",
    "        \n",
    "        # 評価結果をエクセルに保存\n",
    "        log_azimuth_wise = {\"干渉音の方向\": interference_azimuth, \"平均実時間比\": rtf_sum/num_file, \\\n",
    "                            \"SDRの平均（混合音）\": np.mean(sdr_mix_list), \"SDRの平均（推定音）\": np.mean(sdr_est_list), \\\n",
    "                            \"SDRの標準偏差（混合音）\": np.std(sdr_mix_list), \"SDRの標準偏差（推定音）\": np.std(sdr_est_list),\\\n",
    "                            \"SIRの平均（混合音）\": np.mean(sir_mix_list), \"SIRの平均（推定音）\": np.mean(sir_est_list), \\\n",
    "                            \"SIRの標準偏差（混合音）\": np.std(sir_mix_list), \"SIRの標準偏差（推定音）\": np.std(sir_est_list), \\\n",
    "                            \"WERの平均（目的音）\": np.mean(wer_clean_list), \"WERの平均（混合音）\": np.mean(wer_mix_list), \"WERの平均（推定音）\": np.mean(wer_est_list), \\\n",
    "                            \"WERの標準偏差（目的音）\": np.std(wer_clean_list), \"WERの標準偏差（混合音）\": np.std(wer_mix_list), \"WERの標準偏差（推定音）\": np.std(wer_est_list), \\\n",
    "                            \"LEの平均（目的音）\": np.mean(localization_error_target_list), \"LEの平均（混合音）\": np.mean(localization_error_mixed_list), \"LEの平均（推定音）\": np.mean(localization_error_estimated_list), \\\n",
    "                            \"LEの標準偏差（目的音）\": np.std(localization_error_target_list), \"LEの標準偏差（混合音）\": np.std(localization_error_mixed_list), \"LEの標準偏差（推定音）\": np.std(localization_error_estimated_list)}\n",
    "        eval_logs.append(log_azimuth_wise)\n",
    "        df = pd.DataFrame(eval_logs)\n",
    "        excel_file_name = \"eval_result_{}_{}_{}_{}_sig_SCM_dereverb_{}.xlsx\".format(test_data_dir.split('/')[-2], denoising_model_type, speaker_separation_model_type, beamformer_type, str(dereverb_type))\n",
    "        log_save_path = os.path.join(checkpoint_path.rsplit('/', maxsplit=1)[0], excel_file_name)\n",
    "        df.to_excel(log_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91b3cee0bd9e13d425ff65d1e2e8c47cb5038befda1924b21f2034e7b82aeda0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('pip': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
