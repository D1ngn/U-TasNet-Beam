{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要モジュールのimport\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal as signal\n",
    "import librosa\n",
    "\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "from models import FCMaskEstimator, BLSTMMaskEstimator, UnetMaskEstimator_kernel3\n",
    "from beamformer import estimate_covariance_matrix, condition_covariance, estimate_steering_vector, sparse, ds_beamformer, mvdr_beamformer, gev_beamformer, mwf\n",
    "from utils import AudioProcess, standardize\n",
    "\n",
    "sys.path.append('..')\n",
    "from MyLibrary.MyFunc import load_audio_file, save_audio_file, wave_plot, audio_eval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azimuth_list: ['0', '15', '30', '45', '60', '75', '90']\n",
      "使用デバイス： cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [1:02:26<3:57:26, 593.59s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 各パラメータを設定\n",
    "    sample_rate = 16000 # 作成するオーディオファイルのサンプリング周波数を指定\n",
    "    audio_length = 3 # 単位は秒(second) → fft_size=1024,hop_length=768のとき、audio_length=6が最適かも？\n",
    "    fft_size = 512 # 高速フーリエ変換のフレームサイズ\n",
    "    hop_length = 160 # 高速フーリエ変換におけるフレームのスライド幅\n",
    "    spec_frame_num = 64 # スペクトログラムのフレーム数 spec_freq_dim=512のとき、音声の長さが5秒の場合は128, 3秒の場合は64\n",
    "    # マスクのチャンネルを指定（いずれはconfigまたはargsで指定）TODO\n",
    "    target_aware_channel = 0\n",
    "    noise_aware_channel = 4\n",
    "    \n",
    "    # 評価する音声ファイルを格納したディレクトリを指定\n",
    "    test_data_dir = \"../data/NoisySpeechDataset_for_unet_fft_512_multi_wav_1209/test/\"\n",
    "    azimuth_list = natsorted(os.listdir(test_data_dir)) # 0, 15, 30,・・・,90\n",
    "#     azimuth_list.pop(1)\n",
    "    print(\"azimuth_list:\", azimuth_list)\n",
    "    \n",
    "    # マスク推定モデルの種類を指定\n",
    "    model_type = 'Unet' # 'FC' or 'BLSTM' or 'Unet'\n",
    "    # ビームフォーマの種類を指定\n",
    "    beamformer_type = 'MVDR' # 'DS' or 'MVDR' or 'GEV', or 'MWF' or 'Sparse'\n",
    "    \n",
    "    # モデルの設定\n",
    "    # 学習済みのパラメータを保存したチェックポイントファイルのパスを指定\n",
    "    checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_aware_1208/ckpt_epoch110.pt\"\n",
    "    # ネットワークモデルを定義\n",
    "    if model_type == 'BLSTM':\n",
    "        model = BLSTMMaskEstimator()\n",
    "    elif model_type == 'FC':\n",
    "        model = FCMaskEstimator()\n",
    "    elif model_type == 'Unet':\n",
    "        model = UnetMaskEstimator_kernel3()\n",
    "        pass\n",
    "    # 前処理クラスのインスタンスを作成\n",
    "    transform = AudioProcess(audio_length, sample_rate, fft_size, hop_length, model_type)\n",
    "    # GPUが使える場合はGPUを使用、使えない場合はCPUを使用\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\" , device)\n",
    "    # 学習済みのパラメータをロード\n",
    "    model_params = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(model_params['model_state_dict'])\n",
    "    # print(\"モデルのパラメータ数：\", count_parameters(model))\n",
    "    # MaskEstimatorを使って推論\n",
    "    # ネットワークを推論モードへ\n",
    "    model.eval()\n",
    "    \n",
    "    # 干渉音の到来方向ごとに評価\n",
    "    for interference_azimuth in azimuth_list:\n",
    "        ######################雑音除去＋音声評価#########################\n",
    "        # 音声評価結果の合計値を格納するリストを用意\n",
    "        sdr_mix_list = []\n",
    "        sir_mix_list = []\n",
    "        sar_mix_list = []\n",
    "        sdr_est_list = []\n",
    "        sir_est_list = []\n",
    "        sar_est_list = []\n",
    "        # 合計処理時間を測るための変数を用意\n",
    "        processing_duration_sum = 0\n",
    "\n",
    "        mixed_audio_path_list = natsorted(glob.glob(os.path.join(test_data_dir, interference_azimuth, \"*_mixed.wav\"))) # （例）p232_016_mixed.wav\n",
    "        for mixed_audio_path in tqdm(mixed_audio_path_list):            \n",
    "            # 処理の開始時間\n",
    "            iter_start_time = time.perf_counter()\n",
    "            # マルチチャンネル音声データを複素スペクトログラムと振幅スペクトログラムに変換\n",
    "            mixed_complex_spec, mixed_amp_spec = transform(mixed_audio_path)\n",
    "            \"\"\"mixed_complex_spec: (num_channels, freq_bins, time_steps), mixed_amp_spec: (num_channels, freq_bins, time_steps)\"\"\"\n",
    "            # 振幅スペクトログラムを標準化\n",
    "            mixed_amp_spec = standardize(mixed_amp_spec)\n",
    "            # numpy形式のデータをpytorchのテンソルに変換\n",
    "            mixed_amp_spec = torch.from_numpy(mixed_amp_spec.astype(np.float32)).clone()\n",
    "            # モデルに入力できるようにバッチサイズの次元を追加\n",
    "            mixed_amp_spec = mixed_amp_spec.unsqueeze(0)\n",
    "            \"\"\"mixed_amp_spec: (batch_size, num_channels, freq_bins, time_steps)\"\"\"\n",
    "            # 音源方向推定情報を含むマスクを推定\n",
    "            target_mask_output, noise_mask_output = model(mixed_amp_spec)\n",
    "            if model_type == 'FC' or 'Unet':\n",
    "                # マスクのチャンネルを指定（目的音に近いチャンネルと雑音に近いチャンネル）\n",
    "                estimated_target_mask = target_mask_output[:, target_aware_channel, :, :]\n",
    "                \"\"\"estimated_target_mask: (batch_size, freq_bins, time_steps)\"\"\"\n",
    "                estimated_noise_mask = noise_mask_output[:, noise_aware_channel, :, :]\n",
    "                \"\"\"estimated_noise_mask: (batch_size, freq_bins, time_steps)\"\"\"\n",
    "            elif model_type == 'BLSTM':\n",
    "                # 複数チャンネル間のマスク値の中央値をとる（median pooling）\n",
    "                (estimated_target_mask, _) = torch.median(target_mask_output, dim=1)\n",
    "                \"\"\"estimated_target_mask: (batch_size, freq_bins, time_steps)\"\"\"\n",
    "                (estimated_noise_mask, _) = torch.median(noise_mask_output, dim=1)\n",
    "                \"\"\"estimated_noise_mask: (batch_size, freq_bins, time_steps)\"\"\"\n",
    "            else:\n",
    "                print(\"Please specify the correct model type\")\n",
    "            # バッチサイズの次元を削除\n",
    "            estimated_target_mask = estimated_target_mask.squeeze(0)\n",
    "            \"\"\"estimated_target_mask: (freq_bins, time_steps)\"\"\"\n",
    "            estimated_noise_mask = estimated_noise_mask.squeeze(0)\n",
    "            \"\"\"estimated_noise_mask: (freq_bins, time_steps)\"\"\"\n",
    "            # U-Netの場合paddingされた分を削除する\n",
    "            if model_type == 'Unet':\n",
    "                # とりあえずハードコーディング TODO\n",
    "                mixed_complex_spec = mixed_complex_spec[:, :, :301]\n",
    "                estimated_target_mask = estimated_target_mask[:, :301] \n",
    "                estimated_noise_mask = estimated_noise_mask[:, :301]\n",
    "\n",
    "            # pytorchのテンソルをnumpy形式のデータに変換\n",
    "            estimated_target_mask = estimated_target_mask.detach().numpy().copy() # CPU\n",
    "            estimated_noise_mask = estimated_noise_mask.detach().numpy().copy() # CPU\n",
    "            # 目的音のマスクと雑音のマスクからそれぞれの空間共分散行列を推定\n",
    "            target_covariance_matrix = estimate_covariance_matrix(mixed_complex_spec, estimated_target_mask)\n",
    "            noise_covariance_matrix = estimate_covariance_matrix(mixed_complex_spec, estimated_noise_mask)\n",
    "            noise_covariance_matrix = condition_covariance(noise_covariance_matrix, 1e-6) # これがないと性能が大きく落ちる（雑音の共分散行列のみで良い）\n",
    "            # noise_covariance_matrix /= np.trace(noise_covariance_matrix, axis1=-2, axis2=-1)[..., None, None]\n",
    "            # ビームフォーマによる雑音除去を実行\n",
    "            if beamformer_type == 'MVDR':\n",
    "                # target_steering_vectors = estimate_steering_vector(target_covariance_matrix)\n",
    "                # estimated_spec = mvdr_beamformer(mixed_complex_spec, target_steering_vectors, noise_covariance_matrix)\n",
    "                estimated_spec = mvdr_beamformer(mixed_complex_spec, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == 'GEV':\n",
    "                estimated_spec = gev_beamformer(mixed_complex_spec, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == \"DS\":\n",
    "                target_steering_vectors = estimate_steering_vector(target_covariance_matrix)\n",
    "                estimated_spec = ds_beamformer(mixed_complex_spec, target_steering_vectors)\n",
    "            elif beamformer_type == \"MWF\":\n",
    "                estimated_spec = mwf(mixed_complex_spec, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == 'Sparse':\n",
    "                estimated_spec = sparse(mixed_complex_spec, estimated_target_mask) # マスクが正常に推定できているかどうかをテストする用\n",
    "            else:\n",
    "                print(\"Please specify the correct beamformer type\")\n",
    "            \"\"\"estimated_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "\n",
    "            # マルチチャンネルスペクトログラムを音声波形に変換\n",
    "            mixed_audio_data = load_audio_file(mixed_audio_path, audio_length, sample_rate)\n",
    "            \"\"\"mixed_audio_data: (num_samples, num_channels)\"\"\"\n",
    "            multichannel_estimated_voice_data= np.zeros(mixed_audio_data.shape, dtype='float64') # マルチチャンネル音声波形を格納する配列\n",
    "            # 1chごとスペクトログラムを音声波形に変換\n",
    "            for i in range(estimated_spec.shape[0]):\n",
    "                estimated_voice_data = librosa.core.istft(estimated_spec[i, :, :], hop_length=hop_length)\n",
    "                multichannel_estimated_voice_data[:, i] = estimated_voice_data\n",
    "            \"\"\"multichannel_estimated_voice_data: (num_samples, num_channels)\"\"\"\n",
    "            # 処理の終了時間\n",
    "            iter_finish_time = time.perf_counter()\n",
    "            # 1ループ当たりの処理時間（音声波形→STFT→雑音除去→iSTFT→音声波形）\n",
    "            iter_processing_duration = iter_finish_time - iter_start_time\n",
    "            processing_duration_sum += iter_processing_duration\n",
    "            \n",
    "            # オーディオデータを保存\n",
    "            estimated_voice_path = \"./estimated_voice.wav\"\n",
    "            save_audio_file(estimated_voice_path, multichannel_estimated_voice_data, sample_rate)\n",
    "            # ファイル名を取得\n",
    "            file_num = os.path.basename(mixed_audio_path).split('.')[0].rsplit('_', maxsplit=1)[0] # （例） p232_016\n",
    "            # 干渉雑音の方位角を取得\n",
    "            target_voice_path = os.path.join(test_data_dir, interference_azimuth, file_num + \"_target.wav\")\n",
    "            interference_audio_path = os.path.join(test_data_dir, interference_azimuth, file_num + \"_interference.wav\") # （例）p232_016_interference.wav\n",
    "            # 音声評価\n",
    "            sdr_mix, sir_mix, sar_mix, sdr_est, sir_est, sar_est = audio_eval(audio_length, sample_rate, \\\n",
    "            target_voice_path, interference_audio_path, mixed_audio_path, estimated_voice_path)\n",
    "            # 音声評価結果を記録\n",
    "            sdr_mix_list.append(sdr_mix)\n",
    "            sir_mix_list.append(sir_mix)\n",
    "            sar_mix_list.append(sar_mix)\n",
    "            sdr_est_list.append(sdr_est)\n",
    "            sir_est_list.append(sir_est)\n",
    "            sar_est_list.append(sar_est)\n",
    "            # 推定音声が蓄積されないように削除\n",
    "            os.remove(estimated_voice_path)\n",
    "\n",
    "        # データの数を取得\n",
    "        num_file = len(mixed_audio_path_list)\n",
    "        print(\"#\" * 50)\n",
    "        print(\"使用デバイス：\" , device)\n",
    "        print(\"干渉音の方向:\", interference_azimuth + 'deg')\n",
    "        print(\"合計処理時間：\", str(processing_duration_sum) + 'sec')\n",
    "        print(\"平均処理時間：\", str(processing_duration_sum/num_file) + 'sec')\n",
    "        print(\"平均 | SDR_mix: {:.3f}, SIR_mix: {:.3f}, SAR_mix: {:.3f}\".format(np.mean(sdr_mix_list), np.mean(sir_mix_list), np.mean(sar_mix_list)))\n",
    "        print(\"平均 | SDR_est: {:.3f}, SIR_est: {:.3f}, SAR_est: {:.3f}\".format(np.mean(sdr_est_list), np.mean(sir_est_list), np.mean(sar_est_list)))\n",
    "        print(\"標準偏差 | SDR_mix: {:.3f}, SIR_mix: {:.3f}, SAR_mix: {:.3f}\".format(np.std(sdr_mix_list), np.std(sir_mix_list), np.std(sar_mix_list)))\n",
    "        print(\"標準偏差 | SDR_est: {:.3f}, SIR_est: {:.3f}, SAR_est: {:.3f}\".format(np.std(sdr_est_list), np.std(sir_est_list), np.std(sar_est_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "custom-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
