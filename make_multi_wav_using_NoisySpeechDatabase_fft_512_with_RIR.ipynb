{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import wave\n",
    "import pyroomacoustics as pa\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 乱数を初期化\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声データをロードし、指定された秒数とサンプリングレートでリサンプル\n",
    "def load_audio_file(file_path, length, sample_rate):\n",
    "    data, sr = sf.read(file_path)\n",
    "    # データが設定値よりも大きい場合は大きさを超えた分をカットする\n",
    "    # データが設定値よりも小さい場合はデータの後ろを0でパディングする\n",
    "    # シングルチャンネル(モノラル)の場合 (data.shape: [num_samples,])\n",
    "    if data.ndim == 1:\n",
    "        if len(data) > sample_rate*length:\n",
    "            data = data[:sample_rate*length]\n",
    "        else:\n",
    "            data = np.pad(data, (0, max(0, sample_rate*length - len(data))), \"constant\")\n",
    "        \"\"\"data: (num_samples, )\"\"\"\n",
    "    # マルチチャンネルの場合 (data.shape: [num_samples, num_channels])\n",
    "    elif data.ndim == 2:\n",
    "        if data.shape[0] > sample_rate*length:\n",
    "            data = data[:sample_rate*length, :]\n",
    "        else:\n",
    "            data = np.pad(data, [(0, max(0, sample_rate*length-data.shape[0])), (0, 0)], \"constant\")\n",
    "        \"\"\"data: (num_samples, num_channels)\"\"\"\n",
    "    else:\n",
    "        print(\"number of audio channels are incorrect\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声データを指定したサンプリングレートで保存\n",
    "def save_audio_file(file_path, data, sample_rate):\n",
    "    # librosa.output.write_wav(file_path, data, sample_rate) # 正常に動作しないので変更\n",
    "    sf.write(file_path, data, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つのオーディオデータを足し合わせる\n",
    "def audio_mixer(data1, data2):\n",
    "    assert len(data1) == len(data2)\n",
    "    mixed_audio = data1 + data2\n",
    "    return mixed_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声データをスペクトログラムに変換する\n",
    "def wave_to_spec(data, fft_size, hop_length):\n",
    "    # 短時間フーリエ変換(STFT)を行い、スペクトログラムを取得\n",
    "    spec = librosa.stft(data, n_fft=fft_size, hop_length=hop_length)\n",
    "    amp = np.abs(spec) # 振幅スペクトログラムを取得\n",
    "    phase = np.exp(1.j * np.angle(spec)) # 位相スペクトログラムを取得(フェーザ表示)\n",
    "    # mel_spec = librosa.feature.melspectrogram(data, sr=sr, n_mels=128) # メルスペクトログラムを用いる場合はこっちを使う\n",
    "    return amp, phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声に室内インパルス応答（Room Impulse Response）を畳み込む\n",
    "def rir_convolve(wave_files, sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR):\n",
    "    \"\"\"\n",
    "    wave_files: シングルチャンネルの音声のパスを格納したリスト\n",
    "    sample_rate: サンプリング周波数 [Hz]\n",
    "    audio_length: 音声の長さ [sec]\n",
    "    doas: 音源の到来方向\n",
    "    distance_mic_to_source: 音源とマイクロホンの距離 [m]\n",
    "    mic_array_loc: マイクロホンアレイの位置座標\n",
    "    R: 各マイクロホンの空間的な座標\n",
    "    room_dim: 部屋の３次元形状を表す（単位はm）\n",
    "    max_order: 部屋の壁で何回音が反射するか（反射しない場合0）\n",
    "    absorption: 部屋の壁でどの程度音が吸収されるか （吸収されない場合None）\n",
    "    SNR: 音声と雑音の比率 [dB]\n",
    "    \"\"\"\n",
    "    n_sources = len(wave_files)\n",
    "#     print(\"音源数:\", n_sources)\n",
    "    source_locations = np.zeros((3, doas.shape[0]), dtype=doas.dtype)\n",
    "    \"\"\"source_locations: (xyz, num_sources)\"\"\"\n",
    "    source_locations[0,  :] = np.cos(doas[:, 1]) * np.cos(doas[:, 0]) # x = rcosφcosθ\n",
    "    source_locations[1,  :] = np.sin(doas[:, 1]) * np.cos(doas[:, 0]) # y = rsinφcosθ\n",
    "    source_locations[2,  :] = np.sin(doas[:, 0]) # z = rsinθ\n",
    "    source_locations *= distance_mic_to_source\n",
    "    source_locations += mic_array_loc[:, None] # マイクロホンアレイからの相対位置→絶対位置\n",
    "    for i in range(n_sources):\n",
    "        x = source_locations[0, i]\n",
    "        y = source_locations[1, i]\n",
    "        z = source_locations[2, i]\n",
    "#         print(\"{}個目の音源の位置： (x, y, z) = ({}, {}, {})\".format(i+1, x, y, z))\n",
    "\n",
    "    # 音源数分の音声ファイルを読み込む\n",
    "    clean_data = np.zeros([n_sources, sample_rate*audio_length], dtype='float64')\n",
    "    for s, wave_file in enumerate(wave_files):\n",
    "        audio_data = load_audio_file(wave_file, audio_length, sample_rate)\n",
    "        clean_data[s, :] = audio_data\n",
    "        \n",
    "    # 部屋を生成する\n",
    "    room = pa.ShoeBox(room_dim, fs=sample_rate, max_order=max_order, absorption=absorption)\n",
    "    # 用いるマイクロホンアレイの情報を設定する\n",
    "    room.add_microphone_array(pa.MicrophoneArray(R, fs=room.fs))\n",
    "    # 各音源をシミュレーションに追加する\n",
    "    for s in range(n_sources):\n",
    "        clean_data[s] /= np.std(clean_data[s])\n",
    "        # たまに「ValueError: The source must be added inside the room.」が出る\n",
    "        room.add_source(source_locations[:, s], signal=clean_data[s])\n",
    "    # RIRのシミュレーション生成と音源信号への畳み込みを実行\n",
    "    room.simulate(snr=SNR)\n",
    "#     # インパルス応答の取得と残響時間（RT60）の取得\n",
    "#     impulse_responses = room.rir\n",
    "#     rt60 = pa.experimental.measure_rt60(impulse_responses[0][0], fs=sampling_rate)\n",
    "#     print(\"残響時間:{} [sec]\".format(rt60))\n",
    "    # 室内インパルス応答を畳み込んだ波形データを取得\n",
    "    convolved_wave = room.mic_array.signals.T\n",
    "    \"\"\"convolved_wave: (num_samples, num_channels)\"\"\"\n",
    "    \n",
    "    return convolved_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "部屋の3次元形状： [5. 5. 5.]\n",
      "マイクロホンアレイ中心座標： [2.5  2.5  0.53]\n",
      "マイクロホン数： 8\n",
      "オリジナルデータの数: 11572\n",
      "trainデータの数: 3500\n",
      "validationデータの数: 350\n",
      "testデータの数: 105\n",
      "trainデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [08:11<00:00,  7.13it/s]\n",
      "  0%|          | 1/350 [00:00<00:37,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validationデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [00:46<00:00,  7.53it/s]\n",
      "  1%|          | 1/105 [00:00<00:10,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:14<00:00,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ作成完了　保存先：../data/NoisySpeechDataset_for_unet_fft_512_multi_wav_1207/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 各パラメータを設定\n",
    "    sample_rate = 16000 # 作成するオーディオファイルのサンプリング周波数を指定\n",
    "    audio_length = 3 # 単位は秒(second) → fft_size=1024,hop_length=768のとき、audio_length=6が最適化かも？\n",
    "    train_val_ratio = 0.9 # trainデータとvalidationデータの割合\n",
    "    fft_size = 512 # 短時間フーリエ変換のフレーム長\n",
    "    hop_length = 160 # 短時間フーリエ変換においてフレームをスライドさせる幅\n",
    "    gain_decay = 0.8 # 音量調整のためのパラメータ（雑音が大きすぎるため）\n",
    "    \n",
    "    # RIR生成用のパラメータ\n",
    "    # 畳み込みに用いる波形\n",
    "    # clean_wave_files = [\"../data/NoisySpeechDatabase/noisy_trainset_28spk_wav_16kHz/p230_013.wav\"]\n",
    "    # 音声と雑音の比率 [dB]\n",
    "    SNR = None\n",
    "    # 音源とマイクロホンの距離 [m]\n",
    "    distance_mic_to_source=2\n",
    "#     # 音源方向（音源が複数ある場合はリストに追加）\n",
    "#     azimuth = [0] # 方位角\n",
    "#     elevation = [np.pi/6] # 仰角\n",
    "    # 部屋（シミュレーション環境）の設定\n",
    "    room_width = 5.0\n",
    "    room_length = 5.0\n",
    "    room_height = 5.0\n",
    "    # 部屋の残響を設定\n",
    "    max_order = 0 # 部屋の壁で何回音が反射するか（反射しない場合0）\n",
    "    absorption = None # 部屋の壁でどの程度音が吸収されるか （吸収されない場合None）\n",
    "    # 以下は固定\n",
    "    # 部屋の３次元形状を表す（単位はm）\n",
    "    room_dim = np.r_[room_width, room_length, room_height]\n",
    "    print(\"部屋の3次元形状：\", room_dim)\n",
    "    # マイクロホンアレイの中心位置\n",
    "    nakbot_height = 0.57 # Nakbotの全長\n",
    "    mic_array_height = nakbot_height - 0.04 # 0.04はTAMAGO-03マイクロホンアレイの頂上部からマイクロホンアレイ中心までの距離\n",
    "    mic_array_loc = np.r_[room_width/2, room_length/2, 0] + [0, 0, mic_array_height] # 部屋の中央に配置されたNakbot上のマイクロホンアレイ\n",
    "    print(\"マイクロホンアレイ中心座標：\", mic_array_loc)\n",
    "    # TAMAGO-03のマイクロホンアレイのマイクロホン配置（単位はm）\n",
    "    mic_alignments = np.array(\n",
    "    [\n",
    "        [0.035, 0.0, 0.0],\n",
    "        [0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, 0.035, 0.0],\n",
    "        [-0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [-0.035, 0.0, 0.0],\n",
    "        [-0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, -0.035, 0.0],\n",
    "        [0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0]\n",
    "    ])\n",
    "    n_channels = np.shape(mic_alignments)[0]\n",
    "    print(\"マイクロホン数：\", n_channels)\n",
    "    # get the microphone array （各マイクロホンの空間的な座標）\n",
    "    R = mic_alignments.T + mic_array_loc[:, None]\n",
    "    \"\"\"R: (3D coordinates [m], num_microphones)\"\"\"\n",
    "#     # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "#     doas = np.array(\n",
    "#     [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#     # [elevation[1], azimuth[1]] # ２個目の音源\n",
    "#     ])\n",
    "    \n",
    "    # データセットを格納するディレクトリを作成\n",
    "    save_dataset_dir = \"../data/NoisySpeechDataset_for_unet_fft_512_multi_wav_1207/\"\n",
    "    os.makedirs(save_dataset_dir, exist_ok=True)\n",
    "\n",
    "    # 学習・評価用\n",
    "    # 人の発話音声のディレクトリを指定\n",
    "    target_data_dir = \"../data/NoisySpeechDatabase/clean_trainset_28spk_wav_16kHz/\"\n",
    "    # 外部雑音のディレクトリを指定\n",
    "    interference_data_dir = \"../data/NoisySpeechDatabase/interference_trainset_wav_16kHz/\"\n",
    "    # 混合音声のディレクトリを指定\n",
    "    mixed_data_dir = \"../data/NoisySpeechDatabase/noisy_trainset_28spk_wav_16kHz/\"\n",
    "\n",
    "    target_data_path_template = os.path.join(target_data_dir, \"*.wav\")\n",
    "    target_list = glob.glob(target_data_path_template)\n",
    "    # データセットをシャッフル\n",
    "    random.shuffle(target_list)\n",
    "    # データをtrainデータとvalidationデータに分割\n",
    "    target_list_for_train = target_list[:int(len(target_list)*train_val_ratio)]\n",
    "    target_list_for_train = random.sample(target_list_for_train, 3500) # データ量削減（7の倍数に指定）\n",
    "    target_list_for_val = target_list[int(len(target_list)*train_val_ratio):]\n",
    "    target_list_for_val = random.sample(target_list_for_val, 350) # データ量削減（7の倍数に指定）\n",
    "    print(\"オリジナルデータの数:\", len(target_list))\n",
    "    print(\"trainデータの数:\", len(target_list_for_train))\n",
    "    print(\"validationデータの数:\", len(target_list_for_val))\n",
    "    \n",
    "    # テスト用\n",
    "    # 人の発話音声のディレクトリを指定\n",
    "    target_data_dir_for_test= \"../data/NoisySpeechDatabase/clean_testset_wav_16kHz/\"\n",
    "    # 外部雑音のディレクトリを指定\n",
    "    interference_data_dir_for_test = \"../data/NoisySpeechDatabase/interference_testset_wav_16kHz/\"\n",
    "    # 混合音声のディレクトリを指定\n",
    "    mixed_data_dir_for_test = \"../data/NoisySpeechDatabase/noisy_testset_wav_16kHz/\"\n",
    "    # テストデータのリストを作成\n",
    "    target_data_path_template_for_test = os.path.join(target_data_dir_for_test, \"*.wav\")\n",
    "    target_list_for_test = glob.glob(target_data_path_template_for_test)\n",
    "    target_list_for_test = random.sample(target_list_for_test, 105) # データ量削減（7の倍数に指定）\n",
    "    print(\"testデータの数:\", len(target_list_for_test))\n",
    "    \n",
    "    # trainデータを作成\n",
    "    print(\"trainデータ作成中\")\n",
    "    train_data_path = os.path.join(save_dataset_dir, \"train\")\n",
    "    os.makedirs(train_data_path, exist_ok=True)\n",
    "    for idx, target_path in enumerate(tqdm(target_list_for_train)):\n",
    "        file_num = os.path.basename(target_path).split('.')[0] # (例)p226_001\n",
    "        target_file_name = file_num + \"_target.wav\" # (例)p226_001_target.wav\n",
    "        \n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        # 目的音の畳み込み\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [0] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        convolved_target_data = rir_convolve([target_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_target_data = convolved_target_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_target_data: (num_samples, num_channels=8)\"\"\"\n",
    "        \n",
    "        # 干渉音の畳み込み\n",
    "        interference_path = os.path.join(interference_data_dir, file_num + \".wav\")\n",
    "        # 干渉音の到来方向を指定（0°, 15°, 30°, 45°, 60°, 75°, 90°の7分割）\n",
    "        interference_azimuth = int(idx / (len(target_list_for_train) / 7)) * (np.pi / 12)\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [interference_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_interference_data = rir_convolve([interference_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_interference_data = convolved_interference_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_interference_data: (num_samples, num_channels=8)\"\"\"\n",
    "#         convolved_interference_data = convolved_interference_data * gain_decay\n",
    "        \n",
    "        # 畳み込む音声をリストに格納\n",
    "        wave_files = [target_path, interference_path]\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [0, interference_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6, np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "        [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_mixed_data = rir_convolve(wave_files, sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分音声データが長くなるので、元に戻す\n",
    "        convolved_mixed_data = convolved_mixed_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_mixed_data: (num_samples, num_channels=8)\"\"\"\n",
    "    \n",
    "        # 混合音声の最大振幅で正規化\n",
    "        normalized_convolved_target_data = convolved_target_data / convolved_mixed_data.max()\n",
    "        normalized_convolved_interference_data = convolved_interference_data / convolved_mixed_data.max()\n",
    "        normalized_convolved_mixed_data = convolved_mixed_data / convolved_mixed_data.max()\n",
    "        # 音声を保存\n",
    "        # 目的音\n",
    "        target_file_path = os.path.join(train_data_path, target_file_name)\n",
    "        save_audio_file(target_file_path, normalized_convolved_target_data, sample_rate)\n",
    "        # 干渉音\n",
    "        interference_file_name = file_num + \"_interference.wav\" # (例)p226_001_interference.wav\n",
    "        interference_file_path = os.path.join(train_data_path, interference_file_name)\n",
    "        save_audio_file(interference_file_path, normalized_convolved_interference_data, sample_rate)\n",
    "        # 混合音声\n",
    "        mixed_file_name = file_num + \"_mixed.wav\" # (例)p226_001_mixed.wav\n",
    "        mixed_file_path = os.path.join(train_data_path, mixed_file_name)\n",
    "        save_audio_file(mixed_file_path, normalized_convolved_mixed_data, sample_rate)\n",
    "        \n",
    "        \n",
    "    # validationデータを作成\n",
    "    print(\"validationデータ作成中\")\n",
    "    val_data_path = os.path.join(save_dataset_dir, \"val\")\n",
    "    os.makedirs(val_data_path, exist_ok=True)\n",
    "    for idx, target_path in enumerate(tqdm(target_list_for_val)):\n",
    "        file_num = os.path.basename(target_path).split('.')[0] # (例)p226_001\n",
    "        target_file_name = file_num + \"_target.wav\" # (例)p226_001_target.wav\n",
    "        \n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        # 目的音の畳み込み\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [0] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        convolved_target_data = rir_convolve([target_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_target_data = convolved_target_data[:sample_rate*audio_length, :]\n",
    "        \n",
    "        # 干渉音の畳み込み\n",
    "        interference_path = os.path.join(interference_data_dir, file_num + \".wav\")\n",
    "        # 干渉音の到来方向を指定（0°, 15°, 30°, 45°, 60°, 75°, 90°の7分割）\n",
    "        interference_azimuth = int(idx / (len(target_list_for_val) / 7)) * (np.pi / 12)\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [interference_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_interference_data = rir_convolve([interference_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_interference_data = convolved_interference_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_interference_data: (num_samples, num_channels=8)\"\"\"\n",
    "#         convolved_interference_data = convolved_interference_data * gain_decay\n",
    "\n",
    "        # 畳み込む音声をリストに格納\n",
    "        wave_files = [target_path, interference_path]\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [0, interference_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6, np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "        [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_mixed_data = rir_convolve(wave_files, sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分音声データが長くなるので、元に戻す\n",
    "        convolved_mixed_data = convolved_mixed_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_mixed_data: (num_samples, num_channels=8)\"\"\"\n",
    "        \n",
    "        # 混合音声の最大振幅で正規化\n",
    "        normalized_convolved_target_data = convolved_target_data / convolved_mixed_data.max()\n",
    "        normalized_convolved_interference_data = convolved_interference_data / convolved_mixed_data.max()\n",
    "        normalized_convolved_mixed_data = convolved_mixed_data / convolved_mixed_data.max()\n",
    "        # 音声を保存\n",
    "        # 目的音\n",
    "        target_file_path = os.path.join(val_data_path, target_file_name)\n",
    "        save_audio_file(target_file_path, normalized_convolved_target_data, sample_rate)\n",
    "        # 干渉音\n",
    "        interference_file_name = file_num + \"_interference.wav\" # (例)p226_001_interference.wav\n",
    "        interference_file_path = os.path.join(val_data_path, interference_file_name)\n",
    "        save_audio_file(interference_file_path, normalized_convolved_interference_data, sample_rate)\n",
    "        # 混合音声\n",
    "        mixed_file_name = file_num + \"_mixed.wav\" # (例)p226_001_mixed.wav\n",
    "        mixed_file_path = os.path.join(val_data_path, mixed_file_name)\n",
    "        save_audio_file(mixed_file_path, normalized_convolved_mixed_data, sample_rate)\n",
    "        \n",
    "        \n",
    "    # testデータを作成\n",
    "    print(\"testデータ作成中\")\n",
    "    test_data_path = os.path.join(save_dataset_dir, \"test\")\n",
    "    os.makedirs(test_data_path, exist_ok=True)\n",
    "    for idx, target_path in enumerate(tqdm(target_list_for_test)):\n",
    "        file_num = os.path.basename(target_path).split('.')[0] # (例)p226_001\n",
    "        target_file_name = file_num + \"_target.wav\" # (例)p226_001_target.wav\n",
    "        \n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        # 目的音の畳み込み\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [0] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        convolved_target_data = rir_convolve([target_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_target_data = convolved_target_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_target_data: (num_samples, num_channels=8)\"\"\"\n",
    "        \n",
    "        # 干渉音の畳み込み\n",
    "        interference_path = os.path.join(interference_data_dir_for_test, file_num + \".wav\")\n",
    "        # 干渉音の到来方向を指定（0°, 15°, 30°, 45°, 60°, 75°, 90°の7分割）\n",
    "        interference_azimuth = int(idx / (len(target_list_for_test) / 7)) * (np.pi / 12)\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [interference_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "#         [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_interference_data = rir_convolve([interference_path], sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_interference_data = convolved_interference_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_interference_data: (num_samples, num_channels=8)\"\"\"\n",
    "#         convolved_interference_data = convolved_interference_data * gain_decay\n",
    "\n",
    "        # 畳み込む音声をリストに格納\n",
    "        wave_files = [target_path, interference_path]\n",
    "        # 音源方向（音源が複数ある場合はリストに追加、目的音の音源方向は固定）\n",
    "        azimuth = [0, interference_azimuth] # 方位角（1個目の音源, 2個目の音源）\n",
    "        elevation = [np.pi/6, np.pi/6] # 仰角（1個目の音源, 2個目の音源）\n",
    "        # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "        doas = np.array(\n",
    "        [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "        [elevation[1], azimuth[1]] # ２個目の音源\n",
    "        ])\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_mixed_data = rir_convolve(wave_files, sample_rate, audio_length, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分音声データが長くなるので、元に戻す\n",
    "        convolved_mixed_data = convolved_mixed_data[:sample_rate*audio_length, :]\n",
    "        \"\"\"convolved_mixed_data: (num_samples, num_channels=8)\"\"\"\n",
    "    \n",
    "        # 混合音声の最大振幅で正規化\n",
    "        normalized_convolved_target_data = convolved_target_data / convolved_mixed_data.max()\n",
    "        normalized_convolved_interference_data = convolved_interference_data / convolved_mixed_data.max()\n",
    "        normalized_convolved_mixed_data = convolved_mixed_data / convolved_mixed_data.max()\n",
    "        # 音声データのサンプリング周波数を指定して保存\n",
    "        # 目的音\n",
    "        target_file_path = os.path.join(test_data_path, target_file_name)\n",
    "        save_audio_file(target_file_path, normalized_convolved_target_data, sample_rate)\n",
    "        # 干渉音\n",
    "        interference_azimuth_degree = str(int(idx / (len(target_list_for_test) / 7)) * 15)\n",
    "        interference_file_name = file_num + \"_interference_azimuth{}.wav\".format(interference_azimuth_degree) # (例)p226_001_interference_azimuth30.wav\n",
    "        interference_file_path = os.path.join(test_data_path, interference_file_name)\n",
    "        save_audio_file(interference_file_path, normalized_convolved_interference_data, sample_rate)\n",
    "        # 混合音声\n",
    "        mixed_file_name = file_num + \"_mixed_azimuth{}.wav\".format(interference_azimuth_degree) # (例)p226_001_mixed_azimuth30.wav\n",
    "        mixed_file_path = os.path.join(test_data_path, mixed_file_name)\n",
    "        save_audio_file(mixed_file_path, normalized_convolved_mixed_data, sample_rate)\n",
    "\n",
    "    print(\"データ作成完了　保存先：{}\".format(save_dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "custom-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
