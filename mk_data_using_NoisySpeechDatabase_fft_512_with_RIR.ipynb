{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import wave\n",
    "import pyroomacoustics as pa\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 乱数を初期化\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　音声データをロードし、指定された秒数とサンプリングレートでリサンプル\n",
    "def load_audio_file(file_path, length, sampling_rate=16000):\n",
    "    data, sr = librosa.load(file_path, sr=sampling_rate)\n",
    "    # data, sr = sf.read(file_path)\n",
    "    # データが設定値よりも大きい場合は大きさを超えた分をカットする\n",
    "    # データが設定値よりも小さい場合はデータの後ろを0でパディングする\n",
    "    if len(data) > sampling_rate*length:\n",
    "        data = data[:sampling_rate*length]\n",
    "    else:\n",
    "        data = np.pad(data, (0, max(0, sampling_rate*length - len(data))), \"constant\")\n",
    "    return data\n",
    "\n",
    "# 音声データを指定したサンプリングレートで保存\n",
    "def save_audio_file(file_path, data, sampling_rate=16000):\n",
    "    # librosa.output.write_wav(file_path, data, sampling_rate) # 正常に動作しないので変更\n",
    "    sf.write(file_path, data, sampling_rate)\n",
    "\n",
    "# 2つのオーディオデータを足し合わせる\n",
    "def audio_mixer(data1, data2):\n",
    "    assert len(data1) == len(data2)\n",
    "    mixed_audio = data1 + data2\n",
    "    return mixed_audio\n",
    "\n",
    "# 音声データをスペクトログラムに変換する\n",
    "def wave_to_spec(data, fft_size, hop_length):\n",
    "    # 短時間フーリエ変換(STFT)を行い、スペクトログラムを取得\n",
    "    spec = librosa.stft(data, n_fft=fft_size, hop_length=hop_length)\n",
    "    mag = np.abs(spec) # 振幅スペクトログラムを取得\n",
    "    phase = np.exp(1.j * np.angle(spec)) # 位相スペクトログラムを取得(フェーザ表示)\n",
    "    # mel_spec = librosa.feature.melspectrogram(data, sr=sr, n_mels=128) # メルスペクトログラムを用いる場合はこっちを使う\n",
    "    return mag, phase\n",
    "\n",
    "\n",
    "# 音声に室内インパルス応答（Room Impulse Response）を畳み込む\n",
    "def rir_convolve(wave_file, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR):\n",
    "    \"\"\"\n",
    "    wave_file: 入力音声のパス\n",
    "    sampling_rate: サンプリング周波数[Hz]\n",
    "    doas: 音源の到来方向\n",
    "    distance_mic_to_source: 音源とマイクロホンの距離 [m]\n",
    "    mic_array_loc: マイクロホンアレイの位置座標\n",
    "    R: 各マイクロホンの空間的な座標\n",
    "    room_dim: 部屋の３次元形状を表す（単位はm）\n",
    "    max_order: 部屋の壁で何回音が反射するか（反射しない場合0）\n",
    "    absorption: 部屋の壁でどの程度音が吸収されるか （吸収されない場合None）\n",
    "    SNR: 音声と雑音の比率 [dB]\n",
    "    \"\"\"\n",
    "    wave_files = [wave_file] # 後のためにリストに格納\n",
    "    n_sources = len(wave_files)\n",
    "#     print(\"音源数:\", n_sources)\n",
    "    source_locations = np.zeros((3, doas.shape[0]), dtype=doas.dtype)\n",
    "    \"\"\"source_locations: (xyz, num_sources)\"\"\"\n",
    "    source_locations[0,  :] = np.cos(doas[:, 1]) * np.cos(doas[:, 0]) # x = rcosφcosθ\n",
    "    source_locations[1,  :] = np.sin(doas[:, 1]) * np.cos(doas[:, 0]) # y = rsinφcosθ\n",
    "    source_locations[2,  :] = np.sin(doas[:, 0]) # z = rsinθ\n",
    "    source_locations *= distance_mic_to_source\n",
    "    source_locations += mic_array_loc[:, None] # マイクロホンアレイからの相対位置→絶対位置\n",
    "    for i in range(n_sources):\n",
    "        x = source_locations[0, i]\n",
    "        y = source_locations[1, i]\n",
    "        z = source_locations[2, i]\n",
    "#         print(\"{}個目の音源の位置： (x, y, z) = ({}, {}, {})\".format(i+1, x, y, z))\n",
    "    # 音声波形の長さを調べる\n",
    "    n_samples = 0\n",
    "    # ファイルを読み込む\n",
    "    for wave_file in wave_files:\n",
    "        wav = wave.open(wave_file)\n",
    "        if n_samples<wav.getnframes():\n",
    "            n_samples=wav.getnframes()\n",
    "        wav.close()\n",
    "    clean_data = np.zeros([n_sources, n_samples])\n",
    "    # ファイルを読み込む\n",
    "    s = 0\n",
    "    for wave_file in wave_files:\n",
    "        wav = wave.open(wave_file)\n",
    "        data = wav.readframes(wav.getnframes())\n",
    "        data = np.frombuffer(data, dtype=np.int16)\n",
    "        data = data/np.iinfo(np.int16).max\n",
    "        clean_data[s, :wav.getnframes()] = data\n",
    "        wav.close()\n",
    "        s = s+1\n",
    "    # 部屋を生成する\n",
    "    room = pa.ShoeBox(room_dim, fs=sampling_rate, max_order=max_order, absorption=absorption)\n",
    "    #　用いるマイクロホンアレイの情報を設定する\n",
    "    room.add_microphone_array(pa.MicrophoneArray(R, fs=room.fs))\n",
    "    # 各音源をシミュレーションに追加する\n",
    "    for s in range(n_sources):\n",
    "        clean_data[s] /= np.std(clean_data[s])\n",
    "        room.add_source(source_locations[:, s], signal=clean_data[s])\n",
    "    # RIRのシミュレーション生成と音源信号への畳み込みを実行\n",
    "    room.simulate(snr=SNR)\n",
    "    convolved_wave = room.mic_array.signals.T/np.max(room.mic_array.signals.T)\n",
    "#     print(\"畳み込み後の音声波形:\", convolved_wave.shape)\n",
    "    # インパルス応答の取得と残響時間（RT60）の取得\n",
    "    impulse_responses = room.rir\n",
    "    rt60 = pa.experimental.measure_rt60(impulse_responses[0][0], fs=sampling_rate)\n",
    "#     print(\"残響時間:{} [sec]\".format(rt60))\n",
    "    \n",
    "    return convolved_wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "部屋の3次元形状： [5. 5. 5.]\n",
      "マイクロホンアレイ中心座標： [2.5  2.5  0.53]\n",
      "マイクロホン数： 8\n",
      "オリジナルデータの数: 11572\n",
      "trainデータの数: 3000\n",
      "validationデータの数: 300\n",
      "testデータの数: 100\n",
      "trainデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [16:38<00:00,  3.01it/s]\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validationデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:40<00:00,  2.98it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testデータ作成中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:24<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ作成完了　保存先：../data/NoisySpeechDataset_for_unet_fft_512_8ch_1007/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 各パラメータを設定\n",
    "    sampling_rate = 16000 # 作成するオーディオファイルのサンプリング周波数を指定\n",
    "    audio_length = 3 # 単位は秒(second) → fft_size=1024,hop_length=768のとき、audio_length=6が最適化かも？\n",
    "    train_val_ratio = 0.9 # trainデータとvalidationデータの割合\n",
    "    fft_size = 512 # 高速フーリエ変換のフレームサイズ\n",
    "    hop_length = 160 # 高速フーリエ変換においてフレームをスライドさせる幅\n",
    "    \n",
    "    # RIR生成用のパラメータ\n",
    "    # 畳み込みに用いる波形\n",
    "    # clean_wave_files = [\"../data/NoisySpeechDatabase/noisy_trainset_28spk_wav_16kHz/p230_013.wav\"]\n",
    "    # 音声と雑音の比率 [dB]\n",
    "    SNR = None\n",
    "    # 音源とマイクロホンの距離 [m]\n",
    "    distance_mic_to_source=2. \n",
    "    # 音源方向（音源が複数ある場合はリストに追加）\n",
    "    azimuth = [0] # 方位角\n",
    "    elevation = [np.pi/6] # 仰角\n",
    "    # 部屋（シミュレーション環境）の設定\n",
    "    room_width = 5.0\n",
    "    room_length = 5.0\n",
    "    room_height = 5.0\n",
    "    # 部屋の残響を設定\n",
    "    max_order = 0 #　部屋の壁で何回音が反射するか（反射しない場合0）\n",
    "    absorption = None # 部屋の壁でどの程度音が吸収されるか （吸収されない場合None）\n",
    "    # 以下は固定\n",
    "    # 部屋の３次元形状を表す（単位はm）\n",
    "    room_dim = np.r_[room_width, room_length, room_height]\n",
    "    print(\"部屋の3次元形状：\", room_dim)\n",
    "    # マイクロホンアレイの中心位置\n",
    "    nakbot_height = 0.57 # Nakbotの全長\n",
    "    mic_array_height = nakbot_height - 0.04 # 0.04はTAMAGO-03マイクロホンアレイの頂上部からマイクロホンアレイ中心までの距離\n",
    "    mic_array_loc = np.r_[room_width/2, room_length/2, 0] + [0, 0, mic_array_height] # 部屋の中央に配置されたNakbot上のマイクロホンアレイ\n",
    "    print(\"マイクロホンアレイ中心座標：\", mic_array_loc)\n",
    "    # TAMAGO-03のマイクロホンアレイのマイクロホン配置（単位はm）\n",
    "    mic_alignments = np.array(\n",
    "    [\n",
    "        [0.035, 0.0, 0.0],\n",
    "        [0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, 0.035, 0.0],\n",
    "        [-0.035/np.sqrt(2), 0.035/np.sqrt(2), 0.0],\n",
    "        [-0.035, 0.0, 0.0],\n",
    "        [-0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0],\n",
    "        [0.0, -0.035, 0.0],\n",
    "        [0.035/np.sqrt(2), -0.035/np.sqrt(2), 0.0]\n",
    "    ])\n",
    "    n_channels = np.shape(mic_alignments)[0]\n",
    "    print(\"マイクロホン数：\", n_channels)\n",
    "    # get the microphone array　（各マイクロホンの空間的な座標）\n",
    "    R = mic_alignments.T + mic_array_loc[:, None]\n",
    "    \"\"\"R: (3D coordinates [m], num_microphones)\"\"\"\n",
    "    # 音源の位置（HARK座標系に対応） [仰角θ, 方位角φ]\n",
    "    doas = np.array(\n",
    "    [[elevation[0], azimuth[0]], # １個目の音源 \n",
    "    # [elevation[1], azimuth[1]] # ２個目の音源\n",
    "    ])\n",
    "    \n",
    "    # データセットを格納するディレクトリを作成\n",
    "    save_dataset_dir = \"../data/NoisySpeechDataset_for_unet_fft_512_8ch_1007/\"\n",
    "    os.makedirs(save_dataset_dir, exist_ok=True)\n",
    "\n",
    "    # 学習・評価用\n",
    "    # 人の発話音声のディレクトリを指定\n",
    "    target_data_dir = \"../data/NoisySpeechDatabase/clean_trainset_28spk_wav_16kHz/\"\n",
    "    # 外部雑音のディレクトリを指定\n",
    "    interference_data_dir = \"../data/NoisySpeechDatabase/interference_trainset_wav_16kHz/\"\n",
    "    # 混合音声のディレクトリを指定\n",
    "    mixed_data_dir = \"../data/NoisySpeechDatabase/noisy_trainset_28spk_wav_16kHz/\"\n",
    "\n",
    "    target_data_path_template = os.path.join(target_data_dir, \"*.wav\")\n",
    "    target_list = glob.glob(target_data_path_template)\n",
    "    # データセットをシャッフル\n",
    "    random.shuffle(target_list)\n",
    "    # データをtrainデータとvalidationデータに分割\n",
    "    target_list_for_train = target_list[:int(len(target_list)*train_val_ratio)]\n",
    "    target_list_for_train = random.sample(target_list_for_train, 3000) # データ量削減\n",
    "    target_list_for_val = target_list[int(len(target_list)*train_val_ratio):]\n",
    "    target_list_for_val = random.sample(target_list_for_val, 300) # データ量削減\n",
    "    print(\"オリジナルデータの数:\", len(target_list))\n",
    "    print(\"trainデータの数:\", len(target_list_for_train))\n",
    "    print(\"validationデータの数:\", len(target_list_for_val))\n",
    "    \n",
    "    # テスト用\n",
    "    # 人の発話音声のディレクトリを指定\n",
    "    target_data_dir_for_test= \"../data/NoisySpeechDatabase/clean_testset_wav_16kHz/\"\n",
    "    # 外部雑音のディレクトリを指定\n",
    "    interference_data_dir_for_test = \"../data/NoisySpeechDatabase/interference_testset_wav_16kHz/\"\n",
    "    # 混合音声のディレクトリを指定\n",
    "    mixed_data_dir_for_test = \"../data/NoisySpeechDatabase/noisy_testset_wav_16kHz/\"\n",
    "    # テストデータのリストを作成\n",
    "    target_data_path_template_for_test = os.path.join(target_data_dir_for_test, \"*.wav\")\n",
    "    target_list_for_test = glob.glob(target_data_path_template_for_test)\n",
    "    target_list_for_test = random.sample(target_list_for_test, 100) # データ量削減\n",
    "    print(\"testデータの数:\", len(target_list_for_test))\n",
    "    \n",
    "    # trainデータを作成\n",
    "    print(\"trainデータ作成中\")\n",
    "    train_data_path = os.path.join(save_dataset_dir, \"train\")\n",
    "    os.makedirs(train_data_path, exist_ok=True)\n",
    "    for target_path in tqdm(target_list_for_train):\n",
    "        file_num = os.path.basename(target_path).split('.')[0] # (例)p226_001\n",
    "        target_file_name = file_num + \"_target.npy\" # (例)p226_001_target.npy\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_target_data = rir_convolve(target_path, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_target_data = convolved_target_data[:sampling_rate*audio_length, :]\n",
    "        \"\"\"convolved_target_data: (num_samples, num_channels=8)\"\"\"\n",
    "        # マルチチャンネルオーディオデータをスペクトログラムに変換\n",
    "        multichannel_target_spec_mag = []\n",
    "        for i in range(convolved_target_data.shape[1]):\n",
    "            # オーディオデータをスペクトログラムに変換\n",
    "            convolved_target_mag, _ = wave_to_spec(convolved_target_data[:, i], fft_size, hop_length)\n",
    "            multichannel_target_spec_mag.append(convolved_target_mag)\n",
    "        multichannel_target_spec_mag = np.array(multichannel_target_spec_mag)\n",
    "        \"\"\"mulichannel_target_spec_mag: (num_channels=8, freq_bins=257, time_frames=301)\"\"\"\n",
    "        max_mag = multichannel_target_spec_mag.max()\n",
    "        normed_multichannel_target_spec_mag = multichannel_target_spec_mag / max_mag\n",
    "        # .npy形式でスペクトログラムを保存\n",
    "        target_save_path = os.path.join(train_data_path, target_file_name)\n",
    "        np.save(target_save_path, normed_multichannel_target_spec_mag)\n",
    "        # 混合音声の処理\n",
    "        mixed_path = os.path.join(mixed_data_dir, file_num + \".wav\")\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_mixed_data = rir_convolve(mixed_path, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_mixed_data = convolved_mixed_data[:sampling_rate*audio_length, :]\n",
    "        # オーディオデータをスペクトログラムに変換\n",
    "        multichannel_mixed_spec_mag = []\n",
    "        for i in range(convolved_mixed_data.shape[1]):\n",
    "            # オーディオデータをスペクトログラムに変換\n",
    "            convolved_mixed_mag, _ = wave_to_spec(convolved_mixed_data[:, i], fft_size, hop_length)\n",
    "            multichannel_mixed_spec_mag.append(convolved_mixed_mag)\n",
    "        multichannel_mixed_spec_mag = np.array(multichannel_mixed_spec_mag)\n",
    "        \"\"\"multichannel_mixed_spec_mag: (num_channels=8, freq_bins=257, time_frames=301)\"\"\"\n",
    "        normed_multichannel_mixed_spec_mag = multichannel_mixed_spec_mag / max_mag\n",
    "        # .npy形式でスペクトログラムを保存\n",
    "        mixed_file_name = file_num + \"_mixed.npy\" # (例)p226_001_mixed.npy\n",
    "        mixed_save_path = os.path.join(train_data_path, mixed_file_name)\n",
    "        np.save(mixed_save_path, normed_multichannel_mixed_spec_mag)\n",
    "\n",
    "    # validationデータを作成\n",
    "    print(\"validationデータ作成中\")\n",
    "    val_data_path = os.path.join(save_dataset_dir, \"val\")\n",
    "    os.makedirs(val_data_path, exist_ok=True)\n",
    "    for target_path in tqdm(target_list_for_val):\n",
    "        file_num = os.path.basename(target_path).split('.')[0] # (例)p226_001\n",
    "        target_file_name = file_num + \"_target.npy\" # (例)p226_001_target.npy\n",
    "         # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_target_data = rir_convolve(target_path, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_target_data = convolved_target_data[:sampling_rate*audio_length, :]\n",
    "        # マルチチャンネルオーディオデータをスペクトログラムに変換\n",
    "        multichannel_target_spec_mag = []\n",
    "        for i in range(convolved_target_data.shape[1]):\n",
    "            # オーディオデータをスペクトログラムに変換\n",
    "            convolved_target_mag, _ = wave_to_spec(convolved_target_data[:, i], fft_size, hop_length)\n",
    "            multichannel_target_spec_mag.append(convolved_target_mag)\n",
    "        multichannel_target_spec_mag = np.array(multichannel_target_spec_mag)\n",
    "        \"\"\"mulichannel_target_spec_mag: (num_channels=8, freq_bins=257, time_frames=301)\"\"\"\n",
    "        max_mag = multichannel_target_spec_mag.max()\n",
    "        normed_multichannel_target_spec_mag = multichannel_target_spec_mag / max_mag\n",
    "        # .npy形式でスペクトログラムを保存\n",
    "        target_save_path = os.path.join(val_data_path, target_file_name)\n",
    "        np.save(target_save_path, normed_multichannel_target_spec_mag)\n",
    "        # 混合音声の処理\n",
    "        mixed_path = os.path.join(mixed_data_dir, file_num + \".wav\")\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_mixed_data = rir_convolve(mixed_path, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_mixed_data = convolved_mixed_data[:sampling_rate*audio_length, :]\n",
    "        # オーディオデータをスペクトログラムに変換\n",
    "        multichannel_mixed_spec_mag = []\n",
    "        for i in range(convolved_mixed_data.shape[1]):\n",
    "            # オーディオデータをスペクトログラムに変換\n",
    "            convolved_mixed_mag, _ = wave_to_spec(convolved_mixed_data[:, i], fft_size, hop_length)\n",
    "            multichannel_mixed_spec_mag.append(convolved_mixed_mag)\n",
    "        multichannel_mixed_spec_mag = np.array(multichannel_mixed_spec_mag)\n",
    "        \"\"\"multichannel_mixed_spec_mag: (num_channels=8, freq_bins=257, time_frames=301)\"\"\"\n",
    "        normed_multichannel_mixed_spec_mag = multichannel_mixed_spec_mag / max_mag\n",
    "        # .npy形式でスペクトログラムを保存\n",
    "        mixed_file_name = file_num + \"_mixed.npy\" # (例)p226_001_mixed.npy\n",
    "        mixed_save_path = os.path.join(val_data_path, mixed_file_name)\n",
    "        np.save(mixed_save_path, normed_multichannel_mixed_spec_mag)\n",
    "        \n",
    "    # testデータを作成\n",
    "    print(\"testデータ作成中\")\n",
    "    test_data_path = os.path.join(save_dataset_dir, \"test\")\n",
    "    os.makedirs(test_data_path, exist_ok=True)\n",
    "    for target_path in tqdm(target_list_for_test):\n",
    "        file_num = os.path.basename(target_path).split('.')[0] # (例)p226_001\n",
    "        target_file_name = file_num + \"_target.wav\" # (例)p226_001_target.wav\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_target_data = rir_convolve(target_path, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_target_data = convolved_target_data[:sampling_rate*audio_length, :]\n",
    "        \"\"\"convolved_target_data: (num_samples, num_channels=8)\"\"\"\n",
    "        # 音声データのサンプリング周波数を指定して保存\n",
    "        target_file_path = os.path.join(test_data_path, target_file_name)\n",
    "        save_audio_file(target_file_path, convolved_target_data, sampling_rate)\n",
    "        # 混合音声の処理\n",
    "        mixed_path = os.path.join(mixed_data_dir_for_test, file_num + \".wav\")\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_mixed_data = rir_convolve(mixed_path, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_mixed_data = convolved_mixed_data[:sampling_rate*audio_length, :]\n",
    "        # 音声データのサンプリング周波数を指定して保存\n",
    "        mixed_file_name = file_num + \"_mixed.wav\" # (例)p226_001_mixed.npy\n",
    "        mixed_file_path = os.path.join(test_data_path, mixed_file_name)\n",
    "        save_audio_file(mixed_file_path, convolved_mixed_data, sampling_rate)\n",
    "         # 干渉音の処理\n",
    "        interference_path = os.path.join(interference_data_dir_for_test, file_num + \".wav\")\n",
    "        # 音声にRIRを畳み込みながらマルチチャンネルに拡張\n",
    "        convolved_interference_data = rir_convolve(interference_path, sampling_rate, doas, distance_mic_to_source, \\\n",
    "                 mic_array_loc, R, room_dim, max_order, absorption, SNR)\n",
    "        # RIRの長さ-1サンプル分オーディオデータが長くなるので、元に戻す\n",
    "        convolved_interference_data = convolved_interference_data[:sampling_rate*audio_length, :]\n",
    "        # 音声データのサンプリング周波数を指定して保存\n",
    "        interference_file_name = file_num + \"_interference.wav\" # (例)p226_001_interference.wav\n",
    "        interference_file_path = os.path.join(test_data_path, interference_file_name)\n",
    "        save_audio_file(interference_file_path, convolved_interference_data, sampling_rate)\n",
    "            \n",
    "\n",
    "    print(\"データ作成完了　保存先：{}\".format(save_dataset_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "custom-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
