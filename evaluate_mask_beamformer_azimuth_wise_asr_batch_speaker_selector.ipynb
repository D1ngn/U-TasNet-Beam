{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "# 必要モジュールのimport\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal as signal\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "\n",
    "from models import FCMaskEstimator, BLSTMMaskEstimator, BLSTMMaskEstimator2, UnetMaskEstimator_kernel3, UnetMaskEstimator_kernel3_single_mask, CNNMaskEstimator_kernel3, UnetMaskEstimator_kernel3_single_mask_two_speakers\n",
    "from beamformer import estimate_covariance_matrix, condition_covariance, estimate_steering_vector, sparse, ds_beamformer, mvdr_beamformer, mvdr_beamformer_two_speakers, gev_beamformer, mwf\n",
    "from utils import AudioProcess, wave_plot\n",
    "# 話者識別用モデル\n",
    "from utilities.embedder import SpeechEmbedder\n",
    "# 音源分離用モジュール \n",
    "from asteroid.models import BaseModel\n",
    "\n",
    "sys.path.append('..')\n",
    "from MyLibrary.MyFunc import audio_eval, ASR, asr_eval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azimuth_list: ['0', '15', '30', '45', '60', '75', '90']\n",
      "使用デバイス： cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using legacy_rel_pos and it will be deprecated in the future.\n",
      "WARNING:root:Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/asteroid_espnet/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore\n",
      "100%|██████████| 30/30 [1:37:27<00:00, 194.92s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "使用デバイス： cpu\n",
      "干渉音の方向：0deg\n",
      "合計処理時間：98.453sec\n",
      "平均処理時間：3.282sec\n",
      "合計実時間比：41.963\n",
      "平均実時間比：1.399\n",
      "============================音源分離性能===============================\n",
      "平均 | SDR_mix: -3.007, SIR_mix: 0.995, SAR_mix: 3.462\n",
      "平均 | SDR_est: 3.758, SIR_est: 6.360, SAR_est: 7.111\n",
      "標準偏差 | SDR_mix: 0.037, SIR_mix: 0.738, SAR_mix: 0.694\n",
      "標準偏差 | SDR_est: 0.598, SIR_est: 1.403, SAR_est: 1.334\n",
      "============================音声認識性能===============================\n",
      "平均 | WER_clean: 10.634\n",
      "平均 | WER_mix: 104.782\n",
      "平均 | WER_est: 82.377\n",
      "標準偏差 | WER_clean: 17.019\n",
      "標準偏差 | WER_mix: 33.162\n",
      "標準偏差 | WER_est: 38.172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:36:23<00:00, 192.77s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "使用デバイス： cpu\n",
      "干渉音の方向：15deg\n",
      "合計処理時間：98.517sec\n",
      "平均処理時間：3.284sec\n",
      "合計実時間比：41.807\n",
      "平均実時間比：1.394\n",
      "============================音源分離性能===============================\n",
      "平均 | SDR_mix: -3.006, SIR_mix: 0.788, SAR_mix: 3.779\n",
      "平均 | SDR_est: 6.872, SIR_est: 17.931, SAR_est: 12.153\n",
      "標準偏差 | SDR_mix: 0.037, SIR_mix: 0.780, SAR_mix: 0.747\n",
      "標準偏差 | SDR_est: 1.473, SIR_est: 2.532, SAR_est: 1.649\n",
      "============================音声認識性能===============================\n",
      "平均 | WER_clean: 10.634\n",
      "平均 | WER_mix: 101.595\n",
      "平均 | WER_est: 13.398\n",
      "標準偏差 | WER_clean: 17.019\n",
      "標準偏差 | WER_mix: 25.958\n",
      "標準偏差 | WER_est: 16.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:36:10<00:00, 192.35s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "使用デバイス： cpu\n",
      "干渉音の方向：30deg\n",
      "合計処理時間：98.028sec\n",
      "平均処理時間：3.268sec\n",
      "合計実時間比：41.756\n",
      "平均実時間比：1.392\n",
      "============================音源分離性能===============================\n",
      "平均 | SDR_mix: -3.006, SIR_mix: 0.767, SAR_mix: 3.843\n",
      "平均 | SDR_est: 6.781, SIR_est: 18.468, SAR_est: 12.306\n",
      "標準偏差 | SDR_mix: 0.038, SIR_mix: 0.621, SAR_mix: 0.687\n",
      "標準偏差 | SDR_est: 1.403, SIR_est: 2.140, SAR_est: 1.382\n",
      "============================音声認識性能===============================\n",
      "平均 | WER_clean: 10.634\n",
      "平均 | WER_mix: 100.067\n",
      "平均 | WER_est: 14.328\n",
      "標準偏差 | WER_clean: 17.019\n",
      "標準偏差 | WER_mix: 28.975\n",
      "標準偏差 | WER_est: 21.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:35:55<00:00, 191.84s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "使用デバイス： cpu\n",
      "干渉音の方向：45deg\n",
      "合計処理時間：99.941sec\n",
      "平均処理時間：3.331sec\n",
      "合計実時間比：42.473\n",
      "平均実時間比：1.416\n",
      "============================音源分離性能===============================\n",
      "平均 | SDR_mix: -3.018, SIR_mix: 0.856, SAR_mix: 3.698\n",
      "平均 | SDR_est: 6.450, SIR_est: 18.149, SAR_est: 11.323\n",
      "標準偏差 | SDR_mix: 0.037, SIR_mix: 0.652, SAR_mix: 0.454\n",
      "標準偏差 | SDR_est: 1.324, SIR_est: 2.893, SAR_est: 1.906\n",
      "============================音声認識性能===============================\n",
      "平均 | WER_clean: 10.634\n",
      "平均 | WER_mix: 95.946\n",
      "平均 | WER_est: 16.034\n",
      "標準偏差 | WER_clean: 17.019\n",
      "標準偏差 | WER_mix: 34.311\n",
      "標準偏差 | WER_est: 27.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:36:32<00:00, 193.10s/it]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "使用デバイス： cpu\n",
      "干渉音の方向：60deg\n",
      "合計処理時間：99.234sec\n",
      "平均処理時間：3.308sec\n",
      "合計実時間比：42.374\n",
      "平均実時間比：1.412\n",
      "============================音源分離性能===============================\n",
      "平均 | SDR_mix: -3.010, SIR_mix: 0.777, SAR_mix: 3.881\n",
      "平均 | SDR_est: 6.421, SIR_est: 17.747, SAR_est: 11.938\n",
      "標準偏差 | SDR_mix: 0.043, SIR_mix: 0.577, SAR_mix: 0.829\n",
      "標準偏差 | SDR_est: 1.427, SIR_est: 2.891, SAR_est: 1.607\n",
      "============================音声認識性能===============================\n",
      "平均 | WER_clean: 10.634\n",
      "平均 | WER_mix: 101.248\n",
      "平均 | WER_est: 11.278\n",
      "標準偏差 | WER_clean: 17.019\n",
      "標準偏差 | WER_mix: 34.276\n",
      "標準偏差 | WER_est: 13.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [2:25:40<00:00, 291.34s/it]   \n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "使用デバイス： cpu\n",
      "干渉音の方向：75deg\n",
      "合計処理時間：99.036sec\n",
      "平均処理時間：3.301sec\n",
      "合計実時間比：42.229\n",
      "平均実時間比：1.408\n",
      "============================音源分離性能===============================\n",
      "平均 | SDR_mix: -3.037, SIR_mix: 0.986, SAR_mix: 3.615\n",
      "平均 | SDR_est: 6.298, SIR_est: 17.921, SAR_est: 11.647\n",
      "標準偏差 | SDR_mix: 0.159, SIR_mix: 0.978, SAR_mix: 0.899\n",
      "標準偏差 | SDR_est: 1.566, SIR_est: 2.984, SAR_est: 2.083\n",
      "============================音声認識性能===============================\n",
      "平均 | WER_clean: 10.634\n",
      "平均 | WER_mix: 101.678\n",
      "平均 | WER_est: 14.410\n",
      "標準偏差 | WER_clean: 17.019\n",
      "標準偏差 | WER_mix: 32.537\n",
      "標準偏差 | WER_est: 19.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:35:38<00:00, 191.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "使用デバイス： cpu\n",
      "干渉音の方向：90deg\n",
      "合計処理時間：98.324sec\n",
      "平均処理時間：3.277sec\n",
      "合計実時間比：41.878\n",
      "平均実時間比：1.396\n",
      "============================音源分離性能===============================\n",
      "平均 | SDR_mix: -2.998, SIR_mix: 1.281, SAR_mix: 3.379\n",
      "平均 | SDR_est: 6.413, SIR_est: 19.117, SAR_est: 11.624\n",
      "標準偏差 | SDR_mix: 0.045, SIR_mix: 0.942, SAR_mix: 0.891\n",
      "標準偏差 | SDR_est: 1.679, SIR_est: 2.805, SAR_est: 1.842\n",
      "============================音声認識性能===============================\n",
      "平均 | WER_clean: 10.634\n",
      "平均 | WER_mix: 93.643\n",
      "平均 | WER_est: 15.032\n",
      "標準偏差 | WER_clean: 17.019\n",
      "標準偏差 | WER_mix: 33.984\n",
      "標準偏差 | WER_est: 27.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 各パラメータを設定\n",
    "    sample_rate = 16000 # 作成するオーディオファイルのサンプリング周波数を指定\n",
    "    fft_size = 512 # 高速フーリエ変換のフレームサイズ\n",
    "    hop_length = 160 # 高速フーリエ変換におけるフレームのスライド幅\n",
    "    # マスクのチャンネルを指定（いずれはconfigまたはargsで指定）TODO\n",
    "    target_aware_channel = 0\n",
    "    noise_aware_channel = 4\n",
    "    # 音声をバッチ処理する際の1バッチ当たりのサンプル数\n",
    "    batch_length = 48000\n",
    "    # 処理後の音声の振幅の最大値を処理前の混合音声の振幅の最大値に合わせる（True）か否か（False）\n",
    "    fit_max_value = False\n",
    "    \n",
    "    # 評価する音声ファイルを格納したディレクトリを指定\n",
    "#     test_data_dir = \"../data/NoisySpeechDataset_for_unet_fft_512_multi_wav_1209/test/\" #　残響なし\n",
    "#     test_data_dir = \"../data/NoisySpeechDataset_for_unet_fft_512_multi_wav_rt0162_1231/test\" # 残響あり（古いバージョン）\n",
    "#     test_data_dir = \"../data/NoisySpeechDataset_for_unet_fft_512_multi_wav_rt0162_20210517/test\" # 残響あり\n",
    "#     test_data_dir = \"../data/NoisySpeechDataset_for_unet_fft_512_multi_wav_reverse_20210226/test\" # 目的音が逆方向（方位角180°方向）から到来\n",
    "#     test_data_dir = \"../data/NoisySpeechDataset_multi_wav_test_original_length_20210526/test\" # 残響なし（テストデータの長さがオリジナル音声と同じ）\n",
    "#     test_data_dir = \"../data/NoisySpeechDataset_multi_wav_test_original_length_rt0161_20210526/test\" # 残響あり（テストデータの長さがオリジナル音声と同じ）\n",
    "#     test_data_dir = \"../data/NoisySpeechDataset_multi_wav_test_original_length_rt0502_20210603/test\" # 残響あり（テストデータの長さがオリジナル音声と同じ）\n",
    "    test_data_dir = \"../data/NoisySpeechDataset_multi_wav_test_original_length_two_speakers_20210604/test\" # 残響なし、複数話者\n",
    "    azimuth_list = natsorted(os.listdir(test_data_dir)) # 0, 15, 30,・・・,90\n",
    "#     azimuth_list.pop(1)\n",
    "    print(\"azimuth_list:\", azimuth_list)\n",
    "    # 音声認識精度評価用正解ラベルを格納したディレクトリを指定\n",
    "    reference_label_dir = \"../data/NoisySpeechDatabase/testset_txt/\"\n",
    "    \n",
    "    # 「https://huggingface.co/models?filter=asteroid」にある話者分離用の学習済みモデルを指定\n",
    "#     pretrained_param_speaker_separation = \"JorisCos/ConvTasNet_Libri2Mix_sepclean_16k\" # ConvTasNet 16kHz\n",
    "    pretrained_param_speaker_separation = \"JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k\" # ConvTasNet 16kHz noisy ← こっちの方が精度が高そう\n",
    "    # 話者識別用の学習済みモデルのパス\n",
    "    embedder_path = \"./utilities/embedder.pt\"\n",
    "    # 声を抽出したい人の発話サンプルのパス\n",
    "    ref_speech_path = \"./sample_audio/p232_123/p232_123_target.wav\"\n",
    "    \n",
    "    # マスク推定モデルの種類を指定\n",
    "    model_type = 'Unet_single_mask' # 'FC' or 'BLSTM' or 'Unet' or 'Unet_single_mask' or 'Unet_single_mask_two_speakers'\n",
    "    # ビームフォーマの種類を指定\n",
    "    beamformer_type = 'MVDR' # 'DS' or 'MVDR' or 'GEV', or 'MWF' or 'Sparse'\n",
    "    # 残響除去手法の種類を指定\n",
    "    dereverb_type = 'None' # None or 'WPE' or 'WPD'\n",
    "    \n",
    "    # 音声認識結果を保存するディレクトリを指定\n",
    "    recog_result_dir = \"./recog_result/{}_{}_{}_{}_dereverb_{}/\".format(test_data_dir.split('/')[-2], model_type, pretrained_param_speaker_separation.split('/')[-1], beamformer_type, str(dereverb_type))\n",
    "    os.makedirs(recog_result_dir, exist_ok=True)\n",
    "    \n",
    "    # モデルの設定\n",
    "    # 学習済みのパラメータを保存したチェックポイントファイルのパスを指定\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_aware_1208/ckpt_epoch110.pt\" # U-Net small\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_aware_1211/ckpt_epoch160.pt\" # U-Net small2\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_all_Unet_aware_1215/ckpt_epoch150.pt\" # U-Net all\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_FC_1201/ckpt_epoch120.pt\" # FC small\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_BLSTM_1201/ckpt_epoch70.pt\" # BLSTM small data（wrong version）\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_BLSTM2_1231/ckpt_epoch100.pt\" # BLSTM2 small data（wrong version）\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_SCM_aware_20210105/ckpt_epoch90.pt\" # U-Net SCM  \n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_aware_20210111/ckpt_epoch100.pt\" # U-Net small data training 1209 dataset (best model)\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_SCM_conditioned_median_20210120/ckpt_epoch110.pt\" # U-Net-SCM median small data training 1209 dataset\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_SCM_raw_conditioned_aware_20210227/ckpt_epoch20.pt\" # U-Net-SCM-raw small data training 1209 dataset\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_CNN_aware_20210310/ckpt_epoch200.pt\" # CNN small data training 1209 dataset\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_BLSTM_median_20210312/ckpt_epoch120.pt\" # BLSTM small data (correct version)\n",
    "    checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_single_mask_median_20210315/ckpt_epoch170.pt\" # U-Net-single-mask small data  (best model new version)\n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_for_unet_fft_512_multi_wav_Unet_single_mask_SCM_conditioned_median_20210318/ckpt_epoch150.pt\" # U-Net-single-mask-SCM small data  \n",
    "#     checkpoint_path = \"./ckpt/ckpt_NoisySpeechDataset_multi_wav_test_original_length_two_speakers_Unet_single_mask_median_lr_000001_20210613/ckpt_epoch500.pt\" # U-Net-single-mask-SCM small data 2speakers\n",
    "   \n",
    "    # ネットワークモデルの定義、チャンネルの選び方の指定、モデル入力時にパディングを行うか否かを指定\n",
    "    if model_type == 'BLSTM':\n",
    "        model = BLSTMMaskEstimator()\n",
    "        channel_select_type = 'median'\n",
    "        padding = False\n",
    "    elif model_type == 'FC':\n",
    "        model = FCMaskEstimator()\n",
    "        channel_select_type = 'aware'\n",
    "        padding = False\n",
    "    elif model_type == 'CNN':\n",
    "        model = CNNMaskEstimator_kernel3()\n",
    "        channel_select_type = 'aware'\n",
    "        padding = True\n",
    "    elif model_type == 'Unet':\n",
    "        model = UnetMaskEstimator_kernel3()\n",
    "        channel_select_type = 'aware'\n",
    "        padding = True\n",
    "    elif model_type == 'Unet_single_mask':\n",
    "        model = UnetMaskEstimator_kernel3_single_mask()\n",
    "        channel_select_type = 'single'\n",
    "        padding = True\n",
    "    elif model_type == 'Unet_single_mask_two_speakers':\n",
    "        model = UnetMaskEstimator_kernel3_single_mask_two_speakers()\n",
    "        channel_select_type = 'single'\n",
    "        padding = True\n",
    "    # 音声処理クラスのインスタンスを作成\n",
    "    audio_processor = AudioProcess(sample_rate, fft_size, hop_length, channel_select_type, padding)\n",
    "    # GPUが使える場合はGPUを使用、使えない場合はCPUを使用\n",
    "#     device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\" , device)\n",
    "    # 学習済みのパラメータをロード\n",
    "    model_params = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(model_params['model_state_dict'])\n",
    "    # print(\"モデルのパラメータ数：\", count_parameters(model))\n",
    "    # MaskEstimatorを使って推論\n",
    "    # ネットワークを推論モードへ\n",
    "    model.eval()\n",
    "    # 音声認識用のインスタンスを生成\n",
    "    asr_ins = ASR(lang='eng')\n",
    "    # 話者分離モデルの学習済みパラメータをダウンロード\n",
    "    speaker_separation_model = BaseModel.from_pretrained(pretrained_param_speaker_separation)\n",
    "    # 話者識別モデルの学習済みパタメータをロード（いずれはhparamsでパラメータを指定できる様にする TODO）\n",
    "    embedder = SpeechEmbedder()\n",
    "    embed_params = torch.load(embedder_path, map_location=device)\n",
    "    embedder.load_state_dict(embed_params)\n",
    "    embedder.eval()\n",
    "    # 声を分離抽出したい人の発話サンプルをロード\n",
    "    ref_speech_data, _ = sf.read(ref_speech_path)\n",
    "    # 発話サンプルの特徴量（ログメルスペクトログラム）をベクトルに変換\n",
    "    ref_complex_spec = audio_processor.calc_complex_spec(ref_speech_data)\n",
    "    ref_log_mel_spec = audio_processor.calc_log_mel_spec(ref_complex_spec)\n",
    "    ref_log_mel_spec = torch.from_numpy(ref_log_mel_spec).float()\n",
    "    ref_dvec = embedder(ref_log_mel_spec[0]) # 入力は1ch分\n",
    "    # PyTorchのテンソルからnumpy配列に変換\n",
    "    ref_dvec = ref_dvec.detach().numpy().copy() # CPU\n",
    "    \"\"\"ref_dvec: (embed_dim=256,)\"\"\"\n",
    "    \n",
    "    # 評価結果のログを保存するリスト\n",
    "    eval_logs = []\n",
    "    # 干渉音の到来方向ごとに評価\n",
    "    for interference_azimuth in azimuth_list:\n",
    "        ######################雑音除去＋音声評価#########################\n",
    "        # 音声評価結果の合計値を格納するリストを用意\n",
    "        sdr_mix_list = []\n",
    "        sir_mix_list = []\n",
    "        sar_mix_list = []\n",
    "        sdr_est_list = []\n",
    "        sir_est_list = []\n",
    "        sar_est_list = []\n",
    "        # 音声認識用を追加\n",
    "        wer_clean_list = []\n",
    "        wer_mix_list = []\n",
    "        wer_est_list = []\n",
    "        \n",
    "        \n",
    "        # 合計処理時間を測るための変数を用意\n",
    "        processing_duration_sum = 0\n",
    "        # 実時間比（Real TIme Factor）を測るための変数を用意\n",
    "        rtf_sum = 0\n",
    "\n",
    "        mixed_audio_path_list = natsorted(glob.glob(os.path.join(test_data_dir, interference_azimuth, \"*_mixed.wav\"))) # （例）p232_016_mixed.wav\n",
    "        for mixed_audio_path in tqdm(mixed_audio_path_list):            \n",
    "            # 処理の開始時間\n",
    "            iter_start_time = time.perf_counter()\n",
    "            # 音声データをロード\n",
    "            mixed_audio_data, _ = sf.read(mixed_audio_path)\n",
    "            \"\"\"mixed_audio_data: (num_samples, num_channels)\"\"\"\n",
    "            mixed_complex_spec = audio_processor.calc_complex_spec(mixed_audio_data)\n",
    "            \"\"\"mixed_complex_spec: (num_channels, freq_bins, time_steps)\"\"\"\n",
    "            # 残響除去手法を指定している場合は残響除去処理を実行\n",
    "            if dereverb_type == 'WPE':\n",
    "                mixed_complex_spec, _ = audio_processor.dereverberation_wpe_multi(mixed_complex_spec)\n",
    "                \n",
    "           # モデルに入力できるように音声をミニバッチに分けながら振幅スペクトログラムに変換\n",
    "            mixed_amp_spec_batch = audio_processor.preprocess_mask_estimator(mixed_audio_data, batch_length)\n",
    "            \"\"\"mixed_amp_spec_batch: (batch_size, num_channels, freq_bins, time_frames)\"\"\"\n",
    "            # 発話とそれ以外の雑音の時間周波数マスクを推定\n",
    "            speech_mask_output, noise_mask_output = model(mixed_amp_spec_batch)\n",
    "            \"\"\"speech_mask_output: (batch_size, num_channels, freq_bins, time_frames), noise_mask_output: (batch_size, num_channels, freq_bins, time_frames)\"\"\"\n",
    "            # ミニバッチに分けられたマスクを時間方向に結合し、混合音にかけて各音源のスペクトログラムを取得\n",
    "            multichannel_speech_spec, _ = audio_processor.postprocess_mask_estimator(mixed_complex_spec, speech_mask_output, batch_length, target_aware_channel)\n",
    "            \"\"\"multichannel_speech_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "            multichannel_noise_spec, estimated_noise_mask = audio_processor.postprocess_mask_estimator(mixed_complex_spec, noise_mask_output, batch_length, noise_aware_channel)\n",
    "            \"\"\"multichannel_noise_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "            # 発話のマルチチャンネルスペクトログラムを音声波形に変換\n",
    "            multichannel_denoised_data = audio_processor.spec_to_wave(multichannel_speech_spec, mixed_audio_data)\n",
    "            \"\"\"multichannel_denoised_data: (num_samples, num_channels)\"\"\"\n",
    "            \n",
    "            # 1ch分を取り出す\n",
    "            multichannel_denoised_data = multichannel_denoised_data[:, 0][:, np.newaxis]\n",
    "            \"\"\"multichannel_denoised_data: (num_samples, num_channels=1)\"\"\"\n",
    "\n",
    "            # 話者分離\n",
    "            separated_audio_data = audio_processor.speaker_separation(speaker_separation_model, multichannel_denoised_data)\n",
    "            \"\"\"separated_audio_data: (num_sources, num_samples, num_channels)\"\"\"\n",
    "            # start_time_speeaker_selector = time.perf_counter()\n",
    "            # 分離音から目的話者の発話を選出（何番目の発話が目的話者のものかを判断）\n",
    "            target_speaker_id, speech_amp_spec_all = audio_processor.speaker_selector(embedder, separated_audio_data, ref_dvec)\n",
    "            \"\"\"speech_amp_spec_all: (num_sources, num_channels, freq_bins, time_frames)\"\"\"\n",
    "            # print(\"ID of the target speaker:\", target_speaker_id)\n",
    "            # finish_time_speeaker_selector = time.perf_counter()\n",
    "            # duration_speeaker_selector = finish_time_speeaker_selector - start_time_speeaker_selector\n",
    "            # rtf = duration_speeaker_selector / (mixed_audio_data.shape[0] / sample_rate)\n",
    "            # print(\"実時間比（Speaker Selector）：{:.3f}\".format(rtf))\n",
    "\n",
    "            # 雑音の振幅スペクトログラムを算出\n",
    "            noise_amp_spec = audio_processor.calc_amp_spec(multichannel_noise_spec)\n",
    "            \"\"\"noise_amp_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "             # IRM計算（将来的にはマスクを使わず、信号から直接空間相関行列を算出できるようにする。あるいはcIRMを使う。） TODO\n",
    "            estimated_target_mask = np.sqrt(speech_amp_spec_all[target_speaker_id] ** 2 / np.maximum((np.sum(speech_amp_spec_all**2, axis=0) + noise_amp_spec ** 2), 1e-7))\n",
    "            \"\"\"estimated_target_mask: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "            estimated_interference_mask = np.zeros_like(estimated_target_mask)\n",
    "            for id in range(speech_amp_spec_all.shape[0]):\n",
    "                # 目的話者以外の話者の発話マスクを足し合わせる\n",
    "                if id == target_speaker_id:\n",
    "                    pass\n",
    "                else:\n",
    "                    estimated_interference_mask += np.sqrt(speech_amp_spec_all[id] ** 2 / np.maximum((np.sum(speech_amp_spec_all**2, axis=0) + noise_amp_spec ** 2), 1e-7))\n",
    "            \"\"\"estimated_interference_mask: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "\n",
    "            # 複数チャンネルのうち1チャンネル分のマスクを算出\n",
    "            if channel_select_type == 'aware':\n",
    "                # 目的音と干渉音に近いチャンネルのマスクをそれぞれ使用（選択するチャンネルを変えて実験してみるのもあり）\n",
    "                estimated_target_mask = estimated_target_mask[target_aware_channel, :, :]\n",
    "                estimated_interference_mask = estimated_interference_mask[noise_aware_channel, :, :]\n",
    "            elif channel_select_type == 'median' or channel_select_type == 'single':\n",
    "                # 複数チャンネル間のマスク値の中央値をとる（median pooling）\n",
    "                estimated_target_mask = np.median(estimated_target_mask, axis=0)\n",
    "                estimated_interference_mask = np.median(estimated_interference_mask, axis=0)\n",
    "            \"\"\"estimated_target_mask: (freq_bins, time_steps), estimated_interference_mask: (freq_bins, time_frames)\"\"\"\n",
    "            \n",
    "            # 目的音のマスクと雑音のマスクからそれぞれの空間共分散行列を推定\n",
    "            target_covariance_matrix = estimate_covariance_matrix(mixed_complex_spec, estimated_target_mask)\n",
    "            interference_covariance_matrix = estimate_covariance_matrix(mixed_complex_spec, estimated_interference_mask)\n",
    "            noise_covariance_matrix = estimate_covariance_matrix(mixed_complex_spec, estimated_noise_mask)\n",
    "            noise_covariance_matrix = condition_covariance(noise_covariance_matrix, 1e-6) # これがないと性能が大きく落ちる（雑音の共分散行列のみで良い）\n",
    "            # noise_covariance_matrix /= np.trace(noise_covariance_matrix, axis1=-2, axis2=-1)[..., None, None]\n",
    "            # ビームフォーマによる雑音除去を実行\n",
    "            if beamformer_type == 'MVDR':\n",
    "                estimated_target_spec = mvdr_beamformer_two_speakers(mixed_complex_spec, target_covariance_matrix, interference_covariance_matrix, noise_covariance_matrix)\n",
    "                # estimated_interference_spec = mvdr_beamformer_two_speakers(mixed_complex_spec, interference_covariance_matrix, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == 'GEV':\n",
    "                estimated_target_spec = gev_beamformer(mixed_complex_spec, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == \"DS\":\n",
    "                target_steering_vectors = estimate_steering_vector(target_covariance_matrix)\n",
    "                estimated_target_spec = ds_beamformer(mixed_complex_spec, target_steering_vectors)\n",
    "            elif beamformer_type == \"MWF\":\n",
    "                estimated_target_spec = mwf(mixed_complex_spec, target_covariance_matrix, noise_covariance_matrix)\n",
    "            elif beamformer_type == 'Sparse':\n",
    "                estimated_target_spec = sparse(mixed_complex_spec, estimated_target_mask) # マスクが正常に推定できているかどうかをテストする用\n",
    "            else:\n",
    "                print(\"Please specify the correct beamformer type\")\n",
    "            \"\"\"estimated_target_spec: (num_channels, freq_bins, time_frames)\"\"\"\n",
    "\n",
    "            # マルチチャンネルスペクトログラムを音声波形に変換\n",
    "            multichannel_estimated_target_voice_data = audio_processor.spec_to_wave(estimated_target_spec, mixed_audio_data)\n",
    "            # multichannel_estimated_interference_voice_data = audio_processor.spec_to_wave(estimated_interference_spec, mixed_audio_data)\n",
    "            \"\"\"multichannel_estimated_target_voice_data: (num_samples, num_channels)\"\"\"\n",
    "            \n",
    "            # 最大値を元の音声に合わせる場合\n",
    "            if fit_max_value:\n",
    "                max_amp_postprocess = multichannel_estimated_voice_data.max()\n",
    "                multichannel_estimated_voice_data *= max_amp_preprocess / max_amp_postprocess\n",
    "                \n",
    "            # 処理の終了時間\n",
    "            iter_finish_time = time.perf_counter()\n",
    "            # 1ループ当たりの処理時間（音声波形→STFT→雑音除去→iSTFT→音声波形）\n",
    "            iter_processing_duration = iter_finish_time - iter_start_time\n",
    "            processing_duration_sum += iter_processing_duration\n",
    "            # 実時間比（Real Time Factor）の算出\n",
    "            iter_rtf = iter_processing_duration / (mixed_audio_data.shape[0] / sample_rate)\n",
    "            rtf_sum += iter_rtf\n",
    "            \n",
    "            # ファイル名を取得\n",
    "            file_num = os.path.basename(mixed_audio_path).split('.')[0].rsplit('_', maxsplit=1)[0] # （例） p232_029_p257_236\n",
    "            target_file_num = file_num.rsplit('_', maxsplit=2)[0] # （例） p232_029\n",
    "            # オーディオデータを保存\n",
    "            estimated_target_voice_path = \"./estimated_target_voice.wav\"\n",
    "#             estimated_target_voice_path = \"./estimated_target_voice/{}/{}_estimated_target_voice.wav\".format(interference_azimuth, file_num)\n",
    "            sf.write(estimated_target_voice_path, multichannel_estimated_target_voice_data, sample_rate)\n",
    "            # 干渉雑音の方位角を取得\n",
    "            target_voice_path = os.path.join(test_data_dir, interference_azimuth, target_file_num + \"_target.wav\")\n",
    "            interference_audio_path = os.path.join(test_data_dir, interference_azimuth, file_num + \"_interference.wav\") # （例）p232_016_interference.wav\n",
    "            \n",
    "            # 音声評価\n",
    "            # 音源分離性能の評価        \n",
    "            sdr_mix, sir_mix, sar_mix, sdr_est, sir_est, sar_est = audio_eval(sample_rate, \\\n",
    "            target_voice_path, interference_audio_path, mixed_audio_path, estimated_target_voice_path)\n",
    "            # 音声評価結果を記録\n",
    "            sdr_mix_list.append(sdr_mix)\n",
    "            sir_mix_list.append(sir_mix)\n",
    "            sar_mix_list.append(sar_mix)\n",
    "            sdr_est_list.append(sdr_est)\n",
    "            sir_est_list.append(sir_est)\n",
    "            sar_est_list.append(sar_est)\n",
    "            # 音声認識性能の評価\n",
    "            # 音声認識を実行\n",
    "            target_voice_recog_text = asr_ins.speech_recognition(target_voice_path) # （例） IT IS MARVELLOUS\n",
    "            target_voice_recog_text = target_voice_recog_text.replace('.', '').replace(',', '').upper().split() # （例） ['IT', 'IS', 'MARVELLOUS']\n",
    "            mixed_audio_recog_text = asr_ins.speech_recognition(mixed_audio_path)\n",
    "            mixed_audio_recog_text = mixed_audio_recog_text.replace('.', '').replace(',', '').upper().split()\n",
    "            estimated_voice_recog_text = asr_ins.speech_recognition(estimated_target_voice_path)\n",
    "            estimated_voice_recog_text = estimated_voice_recog_text.replace('.', '').replace(',', '').upper().split()\n",
    "            # 正解ラベルを読み込む\n",
    "            reference_label_path = os.path.join(reference_label_dir, target_file_num + '.txt')\n",
    "            with open(reference_label_path, 'r', encoding=\"utf8\") as ref:\n",
    "                # ピリオドとコンマを消して大文字に変換した後、スペースで分割\n",
    "                reference_label_text = ref.read().replace('.', '').replace(',', '').upper().split()  \n",
    "            # WERを計算\n",
    "            clean_recog_result_save_path = os.path.join(recog_result_dir, file_num + '_clean.txt')\n",
    "            mix_recog_result_save_path = os.path.join(recog_result_dir, file_num + '_mix.txt')\n",
    "            est_recog_result_save_path = os.path.join(recog_result_dir, file_num + '_est.txt')\n",
    "            wer_clean = asr_eval(reference_label_text, target_voice_recog_text, clean_recog_result_save_path)\n",
    "            wer_mix = asr_eval(reference_label_text, mixed_audio_recog_text, mix_recog_result_save_path)\n",
    "            wer_est = asr_eval(reference_label_text, estimated_voice_recog_text, est_recog_result_save_path)\n",
    "            wer_clean_list.append(wer_clean)\n",
    "            wer_mix_list.append(wer_mix)\n",
    "            wer_est_list.append(wer_est)\n",
    "            # 推定音声が蓄積されないように削除\n",
    "            os.remove(estimated_target_voice_path)\n",
    "\n",
    "        # データの数を取得\n",
    "        num_file = len(mixed_audio_path_list)\n",
    "        print(\"#\" * 50)\n",
    "        print(\"使用デバイス：\" , device)\n",
    "        print(\"干渉音の方向：{}deg\".format(interference_azimuth))\n",
    "        print(\"合計処理時間：{:.3f}sec\".format(processing_duration_sum))\n",
    "        print(\"平均処理時間：{:.3f}sec\".format(processing_duration_sum/num_file))\n",
    "        print(\"合計実時間比：{:.3f}\".format(rtf_sum))\n",
    "        print(\"平均実時間比：{:.3f}\".format(rtf_sum/num_file))\n",
    "        print(\"============================音源分離性能===============================\")\n",
    "        print(\"平均 | SDR_mix: {:.3f}, SIR_mix: {:.3f}, SAR_mix: {:.3f}\".format(np.mean(sdr_mix_list), np.mean(sir_mix_list), np.mean(sar_mix_list)))\n",
    "        print(\"平均 | SDR_est: {:.3f}, SIR_est: {:.3f}, SAR_est: {:.3f}\".format(np.mean(sdr_est_list), np.mean(sir_est_list), np.mean(sar_est_list)))\n",
    "        print(\"標準偏差 | SDR_mix: {:.3f}, SIR_mix: {:.3f}, SAR_mix: {:.3f}\".format(np.std(sdr_mix_list), np.std(sir_mix_list), np.std(sar_mix_list)))\n",
    "        print(\"標準偏差 | SDR_est: {:.3f}, SIR_est: {:.3f}, SAR_est: {:.3f}\".format(np.std(sdr_est_list), np.std(sir_est_list), np.std(sar_est_list)))\n",
    "        print(\"============================音声認識性能===============================\")\n",
    "        print(\"平均 | WER_clean: {:.3f}\".format(np.mean(wer_clean_list)))\n",
    "        print(\"平均 | WER_mix: {:.3f}\".format(np.mean(wer_mix_list)))\n",
    "        print(\"平均 | WER_est: {:.3f}\".format(np.mean(wer_est_list)))\n",
    "        print(\"標準偏差 | WER_clean: {:.3f}\".format(np.std(wer_clean_list)))\n",
    "        print(\"標準偏差 | WER_mix: {:.3f}\".format(np.std(wer_mix_list)))\n",
    "        print(\"標準偏差 | WER_est: {:.3f}\".format(np.std(wer_est_list)))\n",
    "        \n",
    "        # 評価結果をエクセルに保存\n",
    "        log_azimuth_wise = {\"干渉音の方向\": interference_azimuth, \"平均実時間比\": rtf_sum/num_file, \\\n",
    "                            \"SDRの平均（混合音）\": np.mean(sdr_mix_list), \"SDRの平均（推定音）\": np.mean(sdr_est_list), \\\n",
    "                            \"SDRの標準偏差（混合音）\": np.std(sdr_mix_list), \"SDRの標準偏差（推定音）\": np.std(sdr_est_list),\\\n",
    "                            \"SIRの平均（混合音）\": np.mean(sir_mix_list), \"SIRの平均（推定音）\": np.mean(sir_est_list), \\\n",
    "                            \"SIRの標準偏差（混合音）\": np.std(sir_mix_list), \"SIRの標準偏差（推定音）\": np.std(sir_est_list), \\\n",
    "                            \"WERの平均（目的音）\": np.mean(wer_clean_list), \"WERの平均（混合音）\": np.mean(wer_mix_list), \"WERの平均（推定音）\": np.mean(wer_est_list), \\\n",
    "                            \"WERの標準偏差（目的音）\": np.std(wer_clean_list), \"WERの標準偏差（混合音）\": np.std(wer_mix_list), \"WERの標準偏差（推定音）\": np.std(wer_est_list)}\n",
    "        eval_logs.append(log_azimuth_wise)\n",
    "        df = pd.DataFrame(eval_logs)\n",
    "        excel_file_name = \"eval_result_{}_{}_{}_{}_dereverb_{}.xlsx\".format(test_data_dir.split('/')[-2], model_type, pretrained_param_speaker_separation.split('/')[-1], beamformer_type, str(dereverb_type))\n",
    "        log_save_path = os.path.join(checkpoint_path.rsplit('/', maxsplit=1)[0], excel_file_name)\n",
    "        df.to_excel(log_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asteroid_espnet",
   "language": "python",
   "name": "asteroid_espnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}